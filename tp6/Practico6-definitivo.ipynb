{"cells":[{"cell_type":"markdown","metadata":{"id":"1eZK_VU2wBGz"},"source":["# Métodos ensemble:\n","\n","1. Cargar los datos MNIST y dividirlos en un conjunto de entrenamiento, un conjunto de validación y un conjunto de test (por ejemplo, utilizar 50.000 instancias para entrenamiento, 10.000 para validación y 10.000 para pruebas). Luego, entrenar varios clasificadores, como un clasificador Random Forest, un clasificador Extra-Trees y un clasificador SVM. A continuación, intentar combinarlos en un conjunto que supere a cada (Cambiando SVM por HistGradientBoostingClassifier) clasificador individual en el conjunto de validación, utilizando votación soft o hard. Una vez que haya encontrado uno, probarlo en el conjunto de pruebas. ¿Cuánto mejor se desempeña en comparación con los clasificadores individuales?\n","\n","2. Ejecutar los clasificadores individuales del ejercicio anterior para hacer predicciones en el conjunto de validación y crear un nuevo conjunto de entrenamiento con las predicciones resultantes: cada instancia de entrenamiento es un vector que contiene el conjunto de predicciones de todos los clasificadores para una imagen, y el objetivo es la clase de la imagen. Entrenar un clasificador en este nuevo conjunto de entrenamiento. ¡Felicidades, acaba de entrenar un blender, y junto con los clasificadores forma un conjunto de stacking! Ahora evaluar el conjunto en el conjunto de pruebas. Para cada imagen en el conjunto de pruebas, hacer predicciones con todos los clasificadores, y luego alimentar las predicciones al mezclador para obtener las predicciones del conjunto. ¿Cómo se compara con el clasificador de votación que entrenó anteriormente?\n","\n","3. Realice el ejercicio 1. otra vez utilizando los algoritmo XGBoost, LightGBM y CatBoost.\n","\n","# Reducción dimensional\n","\n","1. Cargue el conjunto de datos MNIST (introducido en el capítulo 3) y divídalo en un conjunto de entrenamiento y un conjunto de pruebas (tome las primeras 60,000 instancias para entrenamiento y las 10,000 restantes para test). Entrene un clasificador Random Forest en el conjunto de datos y tome el tiempo que tarda, luego evalúe el modelo resultante en el conjunto de test. A continuación, use PCA para reducir la dimensionalidad del conjunto de datos, con una relación de varianza explicada del 95%. Entrenar un nuevo clasificador Random Forest en el conjunto de datos reducido y ver cuánto tiempo tarda. ¿Fue el entrenamiento mucho más rápido? A continuación, evalúe el clasificador en el conjunto de pruebas. ¿Cómo se compara con el clasificador anterior?\n","\n","2. Use t-SNE para reducir el conjunto de datos MNIST a dos dimensiones y grafique el resultado usando Matplotlib. Puede usar un gráfico de dispersión utilizando 10 colores diferentes para representar la clase objetivo de cada imagen. Alternativamente, puede reemplazar cada punto en el gráfico de dispersión con la clase correspondiente de la instancia (un dígito del 0 al 9), o incluso graficar versiones reducidas de las imágenes de dígitos en sí mismas (si grafica todos los dígitos, la visualización será demasiado desordenada, por lo que debe dibujar una muestra aleatoria o graficar una instancia solo si no se ha graficado otra instancia a una distancia cercana). Debería obtener una visualización con grupos de dígitos bien separados. Intente usar otros algoritmos de reducción de dimensionalidad como PCA, LLE o MDS y compare las visualizaciones resultantes."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"lcRXifnzyOJi"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/juan-pablo/.local/lib/python3.8/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]},{"data":{"text/plain":["0        5\n","1        0\n","2        4\n","3        1\n","4        9\n","        ..\n","69995    2\n","69996    3\n","69997    4\n","69998    5\n","69999    6\n","Name: class, Length: 70000, dtype: uint8"]},"metadata":{},"output_type":"display_data"}],"source":["#importar dataset\n","from sklearn.datasets import fetch_openml\n","import numpy as np\n","\n","data=fetch_openml('mnist_784',version=1)\n","data.keys()\n","X=data['data']\n","y=data['target']\n","\n","y=y.astype(np.uint8)\n","\n","display(y)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#divido el dataset en conjunto de entrenamiento y prueba y validacion con \n","#test size 10000\n","from sklearn.model_selection import train_test_split\n","\n","X_prep, X_test, y_prep, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_prep, y_prep, test_size=10000, random_state=42)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.9701\n"]}],"source":["#instancio el modelo de random forest\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import RandomForestClassifier\n","\n","\n","rnd_clf = RandomForestClassifier()\n","\n","#Entreno el modelo\n","rnd_clf.fit(X_train, y_train)\n","\n","#hago la prediccion con el conjunto de validacion\n","y_pred = rnd_clf.predict(X_val)\n","accuracy=accuracy_score(y_val, y_pred)\n","print(\"Accuracy: \",accuracy)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.972\n"]}],"source":["#Entreno el modelo de extraTrees\n","from sklearn.ensemble import ExtraTreesClassifier\n","\n","\n","ext_clf = ExtraTreesClassifier()\n","#Entreno el modelo\n","ext_clf.fit(X_train, y_train)\n","#hago la prediccion con el conjunto de validacion\n","y_pred_ext = ext_clf.predict(X_val)\n","accuracy_ext=accuracy_score(y_val, y_pred_ext)\n","print(\"Accuracy: \",accuracy_ext)\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8765\n"]},{"name":"stderr","output_type":"stream","text":["/home/juan-pablo/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["#Entreno el modelo lineal SVC. Aunque hay un problema con este\n","#porque arruina el modelo, entonces despues de entrenarlo, entreno\n","#otro modelo de gradientboosting\n","\n","from sklearn.svm import LinearSVC\n","\n","#instancio\n","linearSvm=LinearSVC()\n","#entreno\n","linearSvm.fit(X_train,y_train)\n","#predigo\n","y_pred_linear=linearSvm.predict(X_val)\n","#accuracy\n","accuracy_linear=accuracy_score(y_val, y_pred_linear)\n","print(\"Accuracy: \",accuracy_linear)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","grd_clf = GradientBoostingClassifier()\n","#Entreno\n","grd_clf.fit(X_train, y_train)\n","#predigo\n","y_pred_grd=grd_clf.predict(X_val)\n","#accuracy\n","accuracy_grd=accuracy_score(y_val, y_pred_grd)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.969\n"]}],"source":["#Ahora usamos votingClassigier para combinar los modelos y ver si mejora\n","#usamos el voting hard\n","from sklearn.ensemble import VotingClassifier\n","voting_clf=VotingClassifier(\n","    estimators=[('rnd',rnd_clf),('ext',ext_clf),('grd',grd_clf)],voting='hard')\n","#Entreno \n","voting_clf.fit(X_train,y_train)\n","\n","#predigo\n","y_pred_voting=voting_clf.predict(X_val)\n","#accuracy\n","accuracy_voting=accuracy_score(y_val, y_pred_voting)\n","print(\"Accuracy: \",accuracy_voting)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.9633\n"]}],"source":["#ahora lo hago con el soft voting\n","from sklearn.ensemble import VotingClassifier\n","voting_clf=VotingClassifier(\n","    estimators=[('rnd',rnd_clf),('ext',ext_clf),('grd',grd_clf)],voting='soft')\n","#Entreno \n","voting_clf.fit(X_train,y_train)\n","\n","#predigo\n","y_pred_voting=voting_clf.predict(X_val)\n","#accuracy\n","accuracy_voting=accuracy_score(y_val, y_pred_voting)\n","print(\"Accuracy: \",accuracy_voting)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#hacer predicciones en el conjunto de validación con los clasificadores individuales y crear un nuevo conjunto de entrenamiento con las predicciones resultantes:\n","#hago el predict con los modelos\n","y_pred_ext = ext_clf.predict(X_val)\n","y_pred_grd = grd_clf.predict(X_val)\n","y_pred_rnd = rnd_clf.predict(X_val)\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#Creo el conjunto de entrenamiento con las predicciones\n","X_blender=np.column_stack((y_pred_ext,y_pred_grd,y_pred_rnd))\n","y_blender=y_val\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier()</pre></div></div></div></div></div>"],"text/plain":["ExtraTreesClassifier()"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#Entreno el blender que es un mezclador de los modelos\n","blender=ExtraTreesClassifier()\n","blender.fit(X_blender,y_blender)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#Predigo con el conjunto de prueba\n","y_pred_ext_test = ext_clf.predict(X_test)\n","y_pred_grd_test = grd_clf.predict(X_test)\n","y_pred_rnd_test = rnd_clf.predict(X_test)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy del clasificador Voting en el conjunto de pruebas: 0.9633\n","Accuracy del blender en el conjunto de pruebas: 0.9649\n"]}],"source":["#Creo el conjunto de caracteristicas para el blender y conjunto de prueba\n","X_test_blender=np.column_stack((y_pred_ext_test,y_pred_grd_test,y_pred_rnd_test))\n","\n","#Obtener predicciones del conjunto de prueba con el blender\n","y_pred_blender=blender.predict(X_test_blender)\n","\n","#Evaluo el rendimiento del blender\n","accuracy_blender=accuracy_score(y_test, y_pred_blender)\n","print(\"Accuracy del clasificador Voting en el conjunto de pruebas:\", accuracy_voting)\n","print(\"Accuracy del blender en el conjunto de pruebas:\", accuracy_blender)\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.9774\n"]}],"source":["#1.3\n","#Entreno el modelo de lightgbm\n","from lightgbm import LGBMClassifier\n","\n","lgt=LGBMClassifier()\n","#Entreno\n","lgt.fit(X_train,y_train)\n","#prediccion\n","y_pred_lgt=lgt.predict(X_val)\n","#accuracy\n","accuracy_lgt=accuracy_score(y_val, y_pred_lgt)\n","print(\"Accuracy: \",accuracy_lgt)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Learning rate set to 0.096599\n","0:\tlearn: 2.0228105\ttotal: 823ms\tremaining: 13m 42s\n","1:\tlearn: 1.8088702\ttotal: 1.55s\tremaining: 12m 54s\n","2:\tlearn: 1.6552545\ttotal: 2.26s\tremaining: 12m 30s\n","3:\tlearn: 1.5354487\ttotal: 2.97s\tremaining: 12m 19s\n","4:\tlearn: 1.4316029\ttotal: 3.65s\tremaining: 12m 7s\n","5:\tlearn: 1.3447731\ttotal: 4.36s\tremaining: 12m 2s\n","6:\tlearn: 1.2708088\ttotal: 5.06s\tremaining: 11m 57s\n","7:\tlearn: 1.1902131\ttotal: 5.77s\tremaining: 11m 55s\n","8:\tlearn: 1.1222290\ttotal: 6.49s\tremaining: 11m 54s\n","9:\tlearn: 1.0651841\ttotal: 7.2s\tremaining: 11m 52s\n","10:\tlearn: 1.0103105\ttotal: 7.91s\tremaining: 11m 51s\n","11:\tlearn: 0.9665881\ttotal: 8.61s\tremaining: 11m 48s\n","12:\tlearn: 0.9251001\ttotal: 9.32s\tremaining: 11m 47s\n","13:\tlearn: 0.8869736\ttotal: 10s\tremaining: 11m 44s\n","14:\tlearn: 0.8490203\ttotal: 10.7s\tremaining: 11m 42s\n","15:\tlearn: 0.8139978\ttotal: 11.4s\tremaining: 11m 39s\n","16:\tlearn: 0.7850682\ttotal: 12s\tremaining: 11m 35s\n","17:\tlearn: 0.7548045\ttotal: 12.7s\tremaining: 11m 33s\n","18:\tlearn: 0.7260764\ttotal: 13.4s\tremaining: 11m 31s\n","19:\tlearn: 0.6994610\ttotal: 14.1s\tremaining: 11m 29s\n","20:\tlearn: 0.6758240\ttotal: 14.8s\tremaining: 11m 27s\n","21:\tlearn: 0.6584476\ttotal: 15.4s\tremaining: 11m 24s\n","22:\tlearn: 0.6390292\ttotal: 16.1s\tremaining: 11m 22s\n","23:\tlearn: 0.6183060\ttotal: 16.8s\tremaining: 11m 21s\n","24:\tlearn: 0.6003355\ttotal: 17.4s\tremaining: 11m 19s\n","25:\tlearn: 0.5842959\ttotal: 18.1s\tremaining: 11m 17s\n","26:\tlearn: 0.5675257\ttotal: 18.7s\tremaining: 11m 14s\n","27:\tlearn: 0.5505829\ttotal: 19.4s\tremaining: 11m 13s\n","28:\tlearn: 0.5355201\ttotal: 20.1s\tremaining: 11m 11s\n","29:\tlearn: 0.5217485\ttotal: 20.8s\tremaining: 11m 10s\n","30:\tlearn: 0.5077338\ttotal: 21.4s\tremaining: 11m 9s\n","31:\tlearn: 0.4943949\ttotal: 22.1s\tremaining: 11m 8s\n","32:\tlearn: 0.4839257\ttotal: 22.8s\tremaining: 11m 7s\n","33:\tlearn: 0.4729592\ttotal: 23.5s\tremaining: 11m 6s\n","34:\tlearn: 0.4626353\ttotal: 24.1s\tremaining: 11m 5s\n","35:\tlearn: 0.4516297\ttotal: 24.8s\tremaining: 11m 4s\n","36:\tlearn: 0.4406066\ttotal: 25.5s\tremaining: 11m 4s\n","37:\tlearn: 0.4301777\ttotal: 26.2s\tremaining: 11m 3s\n","38:\tlearn: 0.4220591\ttotal: 26.9s\tremaining: 11m 2s\n","39:\tlearn: 0.4137321\ttotal: 27.6s\tremaining: 11m 1s\n","40:\tlearn: 0.4046760\ttotal: 28.3s\tremaining: 11m 1s\n","41:\tlearn: 0.3974581\ttotal: 29s\tremaining: 11m\n","42:\tlearn: 0.3911356\ttotal: 29.6s\tremaining: 10m 59s\n","43:\tlearn: 0.3838375\ttotal: 30.3s\tremaining: 10m 58s\n","44:\tlearn: 0.3761023\ttotal: 31s\tremaining: 10m 57s\n","45:\tlearn: 0.3705633\ttotal: 31.6s\tremaining: 10m 56s\n","46:\tlearn: 0.3650636\ttotal: 32.3s\tremaining: 10m 55s\n","47:\tlearn: 0.3589824\ttotal: 33s\tremaining: 10m 54s\n","48:\tlearn: 0.3528976\ttotal: 33.7s\tremaining: 10m 53s\n","49:\tlearn: 0.3469640\ttotal: 34.4s\tremaining: 10m 53s\n","50:\tlearn: 0.3411270\ttotal: 35.1s\tremaining: 10m 52s\n","51:\tlearn: 0.3353222\ttotal: 35.9s\tremaining: 10m 53s\n","52:\tlearn: 0.3292450\ttotal: 36.7s\tremaining: 10m 56s\n","53:\tlearn: 0.3252496\ttotal: 37.5s\tremaining: 10m 56s\n","54:\tlearn: 0.3207173\ttotal: 38.2s\tremaining: 10m 56s\n","55:\tlearn: 0.3163562\ttotal: 38.9s\tremaining: 10m 56s\n","56:\tlearn: 0.3123024\ttotal: 39.6s\tremaining: 10m 55s\n","57:\tlearn: 0.3087590\ttotal: 40.3s\tremaining: 10m 54s\n","58:\tlearn: 0.3033582\ttotal: 41s\tremaining: 10m 54s\n","59:\tlearn: 0.3001619\ttotal: 41.8s\tremaining: 10m 54s\n","60:\tlearn: 0.2960684\ttotal: 42.5s\tremaining: 10m 53s\n","61:\tlearn: 0.2940680\ttotal: 43.1s\tremaining: 10m 52s\n","62:\tlearn: 0.2885382\ttotal: 43.9s\tremaining: 10m 52s\n","63:\tlearn: 0.2852716\ttotal: 44.7s\tremaining: 10m 53s\n","64:\tlearn: 0.2820606\ttotal: 45.4s\tremaining: 10m 52s\n","65:\tlearn: 0.2792218\ttotal: 46s\tremaining: 10m 51s\n","66:\tlearn: 0.2751815\ttotal: 46.7s\tremaining: 10m 50s\n","67:\tlearn: 0.2722344\ttotal: 47.4s\tremaining: 10m 50s\n","68:\tlearn: 0.2692355\ttotal: 48.2s\tremaining: 10m 49s\n","69:\tlearn: 0.2666906\ttotal: 48.9s\tremaining: 10m 49s\n","70:\tlearn: 0.2640819\ttotal: 49.5s\tremaining: 10m 48s\n","71:\tlearn: 0.2610104\ttotal: 50.2s\tremaining: 10m 47s\n","72:\tlearn: 0.2580507\ttotal: 51s\tremaining: 10m 47s\n","73:\tlearn: 0.2546840\ttotal: 51.6s\tremaining: 10m 46s\n","74:\tlearn: 0.2519914\ttotal: 52.3s\tremaining: 10m 44s\n","75:\tlearn: 0.2492220\ttotal: 52.9s\tremaining: 10m 43s\n","76:\tlearn: 0.2462138\ttotal: 53.6s\tremaining: 10m 42s\n","77:\tlearn: 0.2428526\ttotal: 54.3s\tremaining: 10m 41s\n","78:\tlearn: 0.2408402\ttotal: 54.9s\tremaining: 10m 40s\n","79:\tlearn: 0.2388259\ttotal: 55.6s\tremaining: 10m 39s\n","80:\tlearn: 0.2375794\ttotal: 56.2s\tremaining: 10m 38s\n","81:\tlearn: 0.2346895\ttotal: 56.9s\tremaining: 10m 37s\n","82:\tlearn: 0.2328650\ttotal: 57.5s\tremaining: 10m 35s\n","83:\tlearn: 0.2306445\ttotal: 58.2s\tremaining: 10m 34s\n","84:\tlearn: 0.2286785\ttotal: 58.8s\tremaining: 10m 33s\n","85:\tlearn: 0.2263729\ttotal: 59.6s\tremaining: 10m 33s\n","86:\tlearn: 0.2243282\ttotal: 1m\tremaining: 10m 32s\n","87:\tlearn: 0.2222771\ttotal: 1m\tremaining: 10m 31s\n","88:\tlearn: 0.2197243\ttotal: 1m 1s\tremaining: 10m 31s\n","89:\tlearn: 0.2180367\ttotal: 1m 2s\tremaining: 10m 30s\n","90:\tlearn: 0.2162734\ttotal: 1m 3s\tremaining: 10m 29s\n","91:\tlearn: 0.2140641\ttotal: 1m 3s\tremaining: 10m 29s\n","92:\tlearn: 0.2118497\ttotal: 1m 4s\tremaining: 10m 28s\n","93:\tlearn: 0.2100148\ttotal: 1m 5s\tremaining: 10m 28s\n","94:\tlearn: 0.2090351\ttotal: 1m 5s\tremaining: 10m 27s\n","95:\tlearn: 0.2069012\ttotal: 1m 6s\tremaining: 10m 26s\n","96:\tlearn: 0.2059088\ttotal: 1m 7s\tremaining: 10m 25s\n","97:\tlearn: 0.2038598\ttotal: 1m 7s\tremaining: 10m 24s\n","98:\tlearn: 0.2023520\ttotal: 1m 8s\tremaining: 10m 24s\n","99:\tlearn: 0.2014112\ttotal: 1m 9s\tremaining: 10m 23s\n","100:\tlearn: 0.2003740\ttotal: 1m 9s\tremaining: 10m 22s\n","101:\tlearn: 0.1993535\ttotal: 1m 10s\tremaining: 10m 21s\n","102:\tlearn: 0.1982303\ttotal: 1m 11s\tremaining: 10m 20s\n","103:\tlearn: 0.1976788\ttotal: 1m 11s\tremaining: 10m 19s\n","104:\tlearn: 0.1962199\ttotal: 1m 12s\tremaining: 10m 18s\n","105:\tlearn: 0.1944078\ttotal: 1m 13s\tremaining: 10m 17s\n","106:\tlearn: 0.1934296\ttotal: 1m 13s\tremaining: 10m 16s\n","107:\tlearn: 0.1916434\ttotal: 1m 14s\tremaining: 10m 15s\n","108:\tlearn: 0.1901440\ttotal: 1m 15s\tremaining: 10m 15s\n","109:\tlearn: 0.1885968\ttotal: 1m 15s\tremaining: 10m 14s\n","110:\tlearn: 0.1878782\ttotal: 1m 16s\tremaining: 10m 13s\n","111:\tlearn: 0.1872925\ttotal: 1m 17s\tremaining: 10m 12s\n","112:\tlearn: 0.1859200\ttotal: 1m 17s\tremaining: 10m 11s\n","113:\tlearn: 0.1852149\ttotal: 1m 18s\tremaining: 10m 10s\n","114:\tlearn: 0.1845359\ttotal: 1m 19s\tremaining: 10m 9s\n","115:\tlearn: 0.1831053\ttotal: 1m 19s\tremaining: 10m 8s\n","116:\tlearn: 0.1819938\ttotal: 1m 20s\tremaining: 10m 7s\n","117:\tlearn: 0.1810621\ttotal: 1m 21s\tremaining: 10m 6s\n","118:\tlearn: 0.1795349\ttotal: 1m 21s\tremaining: 10m 5s\n","119:\tlearn: 0.1783623\ttotal: 1m 22s\tremaining: 10m 4s\n","120:\tlearn: 0.1771094\ttotal: 1m 23s\tremaining: 10m 3s\n","121:\tlearn: 0.1755248\ttotal: 1m 23s\tremaining: 10m 2s\n","122:\tlearn: 0.1745362\ttotal: 1m 24s\tremaining: 10m 1s\n","123:\tlearn: 0.1739908\ttotal: 1m 25s\tremaining: 10m\n","124:\tlearn: 0.1736016\ttotal: 1m 25s\tremaining: 9m 59s\n","125:\tlearn: 0.1728998\ttotal: 1m 26s\tremaining: 9m 58s\n","126:\tlearn: 0.1715312\ttotal: 1m 26s\tremaining: 9m 57s\n","127:\tlearn: 0.1703748\ttotal: 1m 27s\tremaining: 9m 56s\n","128:\tlearn: 0.1687226\ttotal: 1m 28s\tremaining: 9m 55s\n","129:\tlearn: 0.1683413\ttotal: 1m 28s\tremaining: 9m 54s\n","130:\tlearn: 0.1677105\ttotal: 1m 29s\tremaining: 9m 53s\n","131:\tlearn: 0.1668405\ttotal: 1m 30s\tremaining: 9m 52s\n","132:\tlearn: 0.1666139\ttotal: 1m 30s\tremaining: 9m 51s\n","133:\tlearn: 0.1661549\ttotal: 1m 31s\tremaining: 9m 50s\n","134:\tlearn: 0.1658299\ttotal: 1m 31s\tremaining: 9m 49s\n","135:\tlearn: 0.1648519\ttotal: 1m 32s\tremaining: 9m 48s\n","136:\tlearn: 0.1645516\ttotal: 1m 33s\tremaining: 9m 47s\n","137:\tlearn: 0.1643388\ttotal: 1m 33s\tremaining: 9m 45s\n","138:\tlearn: 0.1636883\ttotal: 1m 34s\tremaining: 9m 44s\n","139:\tlearn: 0.1631674\ttotal: 1m 35s\tremaining: 9m 43s\n","140:\tlearn: 0.1629168\ttotal: 1m 35s\tremaining: 9m 42s\n","141:\tlearn: 0.1622434\ttotal: 1m 36s\tremaining: 9m 41s\n","142:\tlearn: 0.1619463\ttotal: 1m 36s\tremaining: 9m 40s\n","143:\tlearn: 0.1614826\ttotal: 1m 37s\tremaining: 9m 39s\n","144:\tlearn: 0.1611260\ttotal: 1m 38s\tremaining: 9m 38s\n","145:\tlearn: 0.1597512\ttotal: 1m 38s\tremaining: 9m 37s\n","146:\tlearn: 0.1587555\ttotal: 1m 39s\tremaining: 9m 36s\n","147:\tlearn: 0.1587002\ttotal: 1m 40s\tremaining: 9m 35s\n","148:\tlearn: 0.1586095\ttotal: 1m 40s\tremaining: 9m 34s\n","149:\tlearn: 0.1576304\ttotal: 1m 41s\tremaining: 9m 33s\n","150:\tlearn: 0.1572325\ttotal: 1m 41s\tremaining: 9m 32s\n","151:\tlearn: 0.1571313\ttotal: 1m 42s\tremaining: 9m 31s\n","152:\tlearn: 0.1565023\ttotal: 1m 43s\tremaining: 9m 30s\n","153:\tlearn: 0.1562706\ttotal: 1m 43s\tremaining: 9m 29s\n","154:\tlearn: 0.1558450\ttotal: 1m 44s\tremaining: 9m 28s\n","155:\tlearn: 0.1557528\ttotal: 1m 44s\tremaining: 9m 27s\n","156:\tlearn: 0.1553373\ttotal: 1m 45s\tremaining: 9m 26s\n","157:\tlearn: 0.1547441\ttotal: 1m 46s\tremaining: 9m 25s\n","158:\tlearn: 0.1543181\ttotal: 1m 46s\tremaining: 9m 25s\n","159:\tlearn: 0.1541691\ttotal: 1m 47s\tremaining: 9m 24s\n","160:\tlearn: 0.1535885\ttotal: 1m 48s\tremaining: 9m 23s\n","161:\tlearn: 0.1535806\ttotal: 1m 48s\tremaining: 9m 22s\n","162:\tlearn: 0.1533616\ttotal: 1m 49s\tremaining: 9m 21s\n","163:\tlearn: 0.1529473\ttotal: 1m 49s\tremaining: 9m 20s\n","164:\tlearn: 0.1527342\ttotal: 1m 50s\tremaining: 9m 19s\n","165:\tlearn: 0.1526274\ttotal: 1m 51s\tremaining: 9m 18s\n","166:\tlearn: 0.1518417\ttotal: 1m 51s\tremaining: 9m 17s\n","167:\tlearn: 0.1516022\ttotal: 1m 52s\tremaining: 9m 16s\n","168:\tlearn: 0.1513981\ttotal: 1m 52s\tremaining: 9m 15s\n","169:\tlearn: 0.1507344\ttotal: 1m 53s\tremaining: 9m 14s\n","170:\tlearn: 0.1506192\ttotal: 1m 54s\tremaining: 9m 13s\n","171:\tlearn: 0.1502992\ttotal: 1m 54s\tremaining: 9m 12s\n","172:\tlearn: 0.1502137\ttotal: 1m 55s\tremaining: 9m 11s\n","173:\tlearn: 0.1498327\ttotal: 1m 56s\tremaining: 9m 11s\n","174:\tlearn: 0.1495784\ttotal: 1m 56s\tremaining: 9m 10s\n","175:\tlearn: 0.1490616\ttotal: 1m 57s\tremaining: 9m 9s\n","176:\tlearn: 0.1489779\ttotal: 1m 57s\tremaining: 9m 8s\n","177:\tlearn: 0.1484519\ttotal: 1m 58s\tremaining: 9m 7s\n","178:\tlearn: 0.1481939\ttotal: 1m 59s\tremaining: 9m 6s\n","179:\tlearn: 0.1477657\ttotal: 1m 59s\tremaining: 9m 5s\n","180:\tlearn: 0.1474010\ttotal: 2m\tremaining: 9m 5s\n","181:\tlearn: 0.1469455\ttotal: 2m 1s\tremaining: 9m 4s\n","182:\tlearn: 0.1465771\ttotal: 2m 1s\tremaining: 9m 3s\n","183:\tlearn: 0.1456972\ttotal: 2m 2s\tremaining: 9m 2s\n","184:\tlearn: 0.1455512\ttotal: 2m 2s\tremaining: 9m 1s\n","185:\tlearn: 0.1448653\ttotal: 2m 3s\tremaining: 9m\n","186:\tlearn: 0.1447331\ttotal: 2m 4s\tremaining: 8m 59s\n","187:\tlearn: 0.1446315\ttotal: 2m 4s\tremaining: 8m 59s\n","188:\tlearn: 0.1444129\ttotal: 2m 5s\tremaining: 8m 58s\n","189:\tlearn: 0.1438992\ttotal: 2m 6s\tremaining: 8m 57s\n","190:\tlearn: 0.1436596\ttotal: 2m 6s\tremaining: 8m 56s\n","191:\tlearn: 0.1434868\ttotal: 2m 7s\tremaining: 8m 55s\n","192:\tlearn: 0.1433211\ttotal: 2m 7s\tremaining: 8m 54s\n","193:\tlearn: 0.1432304\ttotal: 2m 8s\tremaining: 8m 54s\n","194:\tlearn: 0.1431833\ttotal: 2m 9s\tremaining: 8m 53s\n","195:\tlearn: 0.1431271\ttotal: 2m 9s\tremaining: 8m 52s\n","196:\tlearn: 0.1430647\ttotal: 2m 10s\tremaining: 8m 51s\n","197:\tlearn: 0.1424095\ttotal: 2m 10s\tremaining: 8m 50s\n","198:\tlearn: 0.1421846\ttotal: 2m 11s\tremaining: 8m 49s\n","199:\tlearn: 0.1415977\ttotal: 2m 12s\tremaining: 8m 48s\n","200:\tlearn: 0.1412294\ttotal: 2m 12s\tremaining: 8m 48s\n","201:\tlearn: 0.1410259\ttotal: 2m 13s\tremaining: 8m 47s\n","202:\tlearn: 0.1407208\ttotal: 2m 14s\tremaining: 8m 46s\n","203:\tlearn: 0.1400287\ttotal: 2m 14s\tremaining: 8m 46s\n","204:\tlearn: 0.1394457\ttotal: 2m 15s\tremaining: 8m 45s\n","205:\tlearn: 0.1393223\ttotal: 2m 16s\tremaining: 8m 44s\n","206:\tlearn: 0.1392729\ttotal: 2m 16s\tremaining: 8m 44s\n","207:\tlearn: 0.1389433\ttotal: 2m 17s\tremaining: 8m 43s\n","208:\tlearn: 0.1387445\ttotal: 2m 18s\tremaining: 8m 42s\n","209:\tlearn: 0.1386960\ttotal: 2m 18s\tremaining: 8m 41s\n","210:\tlearn: 0.1385225\ttotal: 2m 19s\tremaining: 8m 41s\n","211:\tlearn: 0.1384715\ttotal: 2m 19s\tremaining: 8m 40s\n","212:\tlearn: 0.1382153\ttotal: 2m 20s\tremaining: 8m 39s\n","213:\tlearn: 0.1380900\ttotal: 2m 21s\tremaining: 8m 38s\n","214:\tlearn: 0.1379794\ttotal: 2m 21s\tremaining: 8m 38s\n","215:\tlearn: 0.1378667\ttotal: 2m 22s\tremaining: 8m 37s\n","216:\tlearn: 0.1376786\ttotal: 2m 23s\tremaining: 8m 36s\n","217:\tlearn: 0.1374841\ttotal: 2m 23s\tremaining: 8m 35s\n","218:\tlearn: 0.1373116\ttotal: 2m 24s\tremaining: 8m 35s\n","219:\tlearn: 0.1370614\ttotal: 2m 25s\tremaining: 8m 34s\n","220:\tlearn: 0.1370172\ttotal: 2m 25s\tremaining: 8m 33s\n","221:\tlearn: 0.1367015\ttotal: 2m 26s\tremaining: 8m 32s\n","222:\tlearn: 0.1364861\ttotal: 2m 27s\tremaining: 8m 32s\n","223:\tlearn: 0.1363106\ttotal: 2m 27s\tremaining: 8m 31s\n","224:\tlearn: 0.1359032\ttotal: 2m 28s\tremaining: 8m 30s\n","225:\tlearn: 0.1357706\ttotal: 2m 28s\tremaining: 8m 30s\n","226:\tlearn: 0.1352998\ttotal: 2m 29s\tremaining: 8m 29s\n","227:\tlearn: 0.1351773\ttotal: 2m 30s\tremaining: 8m 28s\n","228:\tlearn: 0.1349698\ttotal: 2m 30s\tremaining: 8m 27s\n","229:\tlearn: 0.1345592\ttotal: 2m 31s\tremaining: 8m 27s\n","230:\tlearn: 0.1345061\ttotal: 2m 32s\tremaining: 8m 26s\n","231:\tlearn: 0.1343514\ttotal: 2m 32s\tremaining: 8m 25s\n","232:\tlearn: 0.1342496\ttotal: 2m 33s\tremaining: 8m 25s\n","233:\tlearn: 0.1341121\ttotal: 2m 34s\tremaining: 8m 24s\n","234:\tlearn: 0.1340897\ttotal: 2m 34s\tremaining: 8m 23s\n","235:\tlearn: 0.1339702\ttotal: 2m 35s\tremaining: 8m 22s\n","236:\tlearn: 0.1339666\ttotal: 2m 35s\tremaining: 8m 21s\n","237:\tlearn: 0.1338876\ttotal: 2m 36s\tremaining: 8m 21s\n","238:\tlearn: 0.1337153\ttotal: 2m 37s\tremaining: 8m 20s\n","239:\tlearn: 0.1334557\ttotal: 2m 37s\tremaining: 8m 19s\n","240:\tlearn: 0.1333803\ttotal: 2m 38s\tremaining: 8m 18s\n","241:\tlearn: 0.1332610\ttotal: 2m 39s\tremaining: 8m 18s\n","242:\tlearn: 0.1332124\ttotal: 2m 39s\tremaining: 8m 17s\n","243:\tlearn: 0.1330425\ttotal: 2m 40s\tremaining: 8m 16s\n","244:\tlearn: 0.1327305\ttotal: 2m 40s\tremaining: 8m 16s\n","245:\tlearn: 0.1320795\ttotal: 2m 41s\tremaining: 8m 15s\n","246:\tlearn: 0.1320387\ttotal: 2m 42s\tremaining: 8m 14s\n","247:\tlearn: 0.1319120\ttotal: 2m 42s\tremaining: 8m 13s\n","248:\tlearn: 0.1317777\ttotal: 2m 43s\tremaining: 8m 13s\n","249:\tlearn: 0.1316631\ttotal: 2m 44s\tremaining: 8m 12s\n","250:\tlearn: 0.1313867\ttotal: 2m 44s\tremaining: 8m 11s\n","251:\tlearn: 0.1313603\ttotal: 2m 45s\tremaining: 8m 10s\n","252:\tlearn: 0.1312241\ttotal: 2m 46s\tremaining: 8m 10s\n","253:\tlearn: 0.1310626\ttotal: 2m 46s\tremaining: 8m 9s\n","254:\tlearn: 0.1302750\ttotal: 2m 47s\tremaining: 8m 8s\n","255:\tlearn: 0.1300470\ttotal: 2m 47s\tremaining: 8m 8s\n","256:\tlearn: 0.1298796\ttotal: 2m 48s\tremaining: 8m 7s\n","257:\tlearn: 0.1298123\ttotal: 2m 49s\tremaining: 8m 6s\n","258:\tlearn: 0.1295067\ttotal: 2m 49s\tremaining: 8m 5s\n","259:\tlearn: 0.1294172\ttotal: 2m 50s\tremaining: 8m 5s\n","260:\tlearn: 0.1291785\ttotal: 2m 51s\tremaining: 8m 4s\n","261:\tlearn: 0.1291518\ttotal: 2m 51s\tremaining: 8m 3s\n","262:\tlearn: 0.1290562\ttotal: 2m 52s\tremaining: 8m 3s\n","263:\tlearn: 0.1289884\ttotal: 2m 53s\tremaining: 8m 2s\n","264:\tlearn: 0.1289253\ttotal: 2m 53s\tremaining: 8m 1s\n","265:\tlearn: 0.1287750\ttotal: 2m 54s\tremaining: 8m 1s\n","266:\tlearn: 0.1286584\ttotal: 2m 54s\tremaining: 8m\n","267:\tlearn: 0.1285639\ttotal: 2m 55s\tremaining: 7m 59s\n","268:\tlearn: 0.1285061\ttotal: 2m 56s\tremaining: 7m 58s\n","269:\tlearn: 0.1284614\ttotal: 2m 56s\tremaining: 7m 57s\n","270:\tlearn: 0.1283663\ttotal: 2m 57s\tremaining: 7m 57s\n","271:\tlearn: 0.1280410\ttotal: 2m 57s\tremaining: 7m 56s\n","272:\tlearn: 0.1279672\ttotal: 2m 58s\tremaining: 7m 55s\n","273:\tlearn: 0.1275855\ttotal: 2m 59s\tremaining: 7m 54s\n","274:\tlearn: 0.1274690\ttotal: 2m 59s\tremaining: 7m 54s\n","275:\tlearn: 0.1273178\ttotal: 3m\tremaining: 7m 53s\n","276:\tlearn: 0.1271624\ttotal: 3m 1s\tremaining: 7m 52s\n","277:\tlearn: 0.1270082\ttotal: 3m 1s\tremaining: 7m 51s\n","278:\tlearn: 0.1269673\ttotal: 3m 2s\tremaining: 7m 51s\n","279:\tlearn: 0.1269651\ttotal: 3m 2s\tremaining: 7m 50s\n","280:\tlearn: 0.1269153\ttotal: 3m 3s\tremaining: 7m 49s\n","281:\tlearn: 0.1267952\ttotal: 3m 4s\tremaining: 7m 48s\n","282:\tlearn: 0.1266799\ttotal: 3m 4s\tremaining: 7m 48s\n","283:\tlearn: 0.1266755\ttotal: 3m 5s\tremaining: 7m 47s\n","284:\tlearn: 0.1266551\ttotal: 3m 5s\tremaining: 7m 46s\n","285:\tlearn: 0.1266197\ttotal: 3m 6s\tremaining: 7m 45s\n","286:\tlearn: 0.1264646\ttotal: 3m 7s\tremaining: 7m 45s\n","287:\tlearn: 0.1263528\ttotal: 3m 7s\tremaining: 7m 44s\n","288:\tlearn: 0.1261564\ttotal: 3m 8s\tremaining: 7m 43s\n","289:\tlearn: 0.1255191\ttotal: 3m 9s\tremaining: 7m 43s\n","290:\tlearn: 0.1252232\ttotal: 3m 9s\tremaining: 7m 42s\n","291:\tlearn: 0.1251925\ttotal: 3m 10s\tremaining: 7m 41s\n","292:\tlearn: 0.1251878\ttotal: 3m 10s\tremaining: 7m 40s\n","293:\tlearn: 0.1251225\ttotal: 3m 11s\tremaining: 7m 40s\n","294:\tlearn: 0.1250482\ttotal: 3m 12s\tremaining: 7m 39s\n","295:\tlearn: 0.1246805\ttotal: 3m 12s\tremaining: 7m 38s\n","296:\tlearn: 0.1245632\ttotal: 3m 13s\tremaining: 7m 37s\n","297:\tlearn: 0.1245480\ttotal: 3m 14s\tremaining: 7m 37s\n","298:\tlearn: 0.1244969\ttotal: 3m 14s\tremaining: 7m 36s\n","299:\tlearn: 0.1243234\ttotal: 3m 15s\tremaining: 7m 35s\n","300:\tlearn: 0.1242384\ttotal: 3m 15s\tremaining: 7m 34s\n","301:\tlearn: 0.1237668\ttotal: 3m 16s\tremaining: 7m 34s\n","302:\tlearn: 0.1235730\ttotal: 3m 17s\tremaining: 7m 33s\n","303:\tlearn: 0.1230694\ttotal: 3m 17s\tremaining: 7m 32s\n","304:\tlearn: 0.1229888\ttotal: 3m 18s\tremaining: 7m 32s\n","305:\tlearn: 0.1228246\ttotal: 3m 19s\tremaining: 7m 31s\n","306:\tlearn: 0.1228221\ttotal: 3m 19s\tremaining: 7m 30s\n","307:\tlearn: 0.1227368\ttotal: 3m 20s\tremaining: 7m 29s\n","308:\tlearn: 0.1227219\ttotal: 3m 20s\tremaining: 7m 29s\n","309:\tlearn: 0.1225869\ttotal: 3m 21s\tremaining: 7m 28s\n","310:\tlearn: 0.1224527\ttotal: 3m 22s\tremaining: 7m 27s\n","311:\tlearn: 0.1223735\ttotal: 3m 22s\tremaining: 7m 27s\n","312:\tlearn: 0.1223432\ttotal: 3m 23s\tremaining: 7m 26s\n","313:\tlearn: 0.1222611\ttotal: 3m 23s\tremaining: 7m 25s\n","314:\tlearn: 0.1222216\ttotal: 3m 24s\tremaining: 7m 24s\n","315:\tlearn: 0.1221859\ttotal: 3m 25s\tremaining: 7m 24s\n","316:\tlearn: 0.1218763\ttotal: 3m 25s\tremaining: 7m 23s\n","317:\tlearn: 0.1218106\ttotal: 3m 26s\tremaining: 7m 22s\n","318:\tlearn: 0.1215940\ttotal: 3m 27s\tremaining: 7m 22s\n","319:\tlearn: 0.1214335\ttotal: 3m 27s\tremaining: 7m 21s\n","320:\tlearn: 0.1214312\ttotal: 3m 28s\tremaining: 7m 20s\n","321:\tlearn: 0.1213611\ttotal: 3m 28s\tremaining: 7m 20s\n","322:\tlearn: 0.1212032\ttotal: 3m 29s\tremaining: 7m 19s\n","323:\tlearn: 0.1208064\ttotal: 3m 30s\tremaining: 7m 18s\n","324:\tlearn: 0.1207849\ttotal: 3m 30s\tremaining: 7m 18s\n","325:\tlearn: 0.1207586\ttotal: 3m 31s\tremaining: 7m 17s\n","326:\tlearn: 0.1205360\ttotal: 3m 32s\tremaining: 7m 16s\n","327:\tlearn: 0.1205077\ttotal: 3m 32s\tremaining: 7m 15s\n","328:\tlearn: 0.1204242\ttotal: 3m 33s\tremaining: 7m 15s\n","329:\tlearn: 0.1201706\ttotal: 3m 33s\tremaining: 7m 14s\n","330:\tlearn: 0.1201120\ttotal: 3m 34s\tremaining: 7m 13s\n","331:\tlearn: 0.1199962\ttotal: 3m 35s\tremaining: 7m 13s\n","332:\tlearn: 0.1199300\ttotal: 3m 35s\tremaining: 7m 12s\n","333:\tlearn: 0.1194127\ttotal: 3m 36s\tremaining: 7m 11s\n","334:\tlearn: 0.1191568\ttotal: 3m 37s\tremaining: 7m 11s\n","335:\tlearn: 0.1189877\ttotal: 3m 37s\tremaining: 7m 10s\n","336:\tlearn: 0.1186391\ttotal: 3m 38s\tremaining: 7m 9s\n","337:\tlearn: 0.1186176\ttotal: 3m 39s\tremaining: 7m 9s\n","338:\tlearn: 0.1185936\ttotal: 3m 39s\tremaining: 7m 8s\n","339:\tlearn: 0.1185087\ttotal: 3m 40s\tremaining: 7m 7s\n","340:\tlearn: 0.1184976\ttotal: 3m 40s\tremaining: 7m 7s\n","341:\tlearn: 0.1183181\ttotal: 3m 41s\tremaining: 7m 6s\n","342:\tlearn: 0.1178395\ttotal: 3m 42s\tremaining: 7m 5s\n","343:\tlearn: 0.1177081\ttotal: 3m 42s\tremaining: 7m 5s\n","344:\tlearn: 0.1177064\ttotal: 3m 43s\tremaining: 7m 4s\n","345:\tlearn: 0.1173066\ttotal: 3m 44s\tremaining: 7m 3s\n","346:\tlearn: 0.1173045\ttotal: 3m 44s\tremaining: 7m 3s\n","347:\tlearn: 0.1172304\ttotal: 3m 45s\tremaining: 7m 2s\n","348:\tlearn: 0.1170621\ttotal: 3m 46s\tremaining: 7m 1s\n","349:\tlearn: 0.1169947\ttotal: 3m 46s\tremaining: 7m 1s\n","350:\tlearn: 0.1168156\ttotal: 3m 47s\tremaining: 7m\n","351:\tlearn: 0.1162142\ttotal: 3m 48s\tremaining: 6m 59s\n","352:\tlearn: 0.1161251\ttotal: 3m 48s\tremaining: 6m 59s\n","353:\tlearn: 0.1160709\ttotal: 3m 49s\tremaining: 6m 58s\n","354:\tlearn: 0.1160422\ttotal: 3m 49s\tremaining: 6m 57s\n","355:\tlearn: 0.1159141\ttotal: 3m 50s\tremaining: 6m 57s\n","356:\tlearn: 0.1158388\ttotal: 3m 51s\tremaining: 6m 56s\n","357:\tlearn: 0.1157977\ttotal: 3m 51s\tremaining: 6m 55s\n","358:\tlearn: 0.1157696\ttotal: 3m 52s\tremaining: 6m 55s\n","359:\tlearn: 0.1155424\ttotal: 3m 53s\tremaining: 6m 54s\n","360:\tlearn: 0.1154478\ttotal: 3m 53s\tremaining: 6m 53s\n","361:\tlearn: 0.1153105\ttotal: 3m 54s\tremaining: 6m 53s\n","362:\tlearn: 0.1150606\ttotal: 3m 55s\tremaining: 6m 52s\n","363:\tlearn: 0.1148938\ttotal: 3m 55s\tremaining: 6m 51s\n","364:\tlearn: 0.1148117\ttotal: 3m 56s\tremaining: 6m 51s\n","365:\tlearn: 0.1146185\ttotal: 3m 57s\tremaining: 6m 50s\n","366:\tlearn: 0.1144267\ttotal: 3m 57s\tremaining: 6m 49s\n","367:\tlearn: 0.1143597\ttotal: 3m 58s\tremaining: 6m 49s\n","368:\tlearn: 0.1143305\ttotal: 3m 58s\tremaining: 6m 48s\n","369:\tlearn: 0.1142489\ttotal: 3m 59s\tremaining: 6m 47s\n","370:\tlearn: 0.1141688\ttotal: 4m\tremaining: 6m 47s\n","371:\tlearn: 0.1140769\ttotal: 4m\tremaining: 6m 46s\n","372:\tlearn: 0.1140330\ttotal: 4m 1s\tremaining: 6m 45s\n","373:\tlearn: 0.1139973\ttotal: 4m 2s\tremaining: 6m 45s\n","374:\tlearn: 0.1135256\ttotal: 4m 2s\tremaining: 6m 44s\n","375:\tlearn: 0.1134811\ttotal: 4m 3s\tremaining: 6m 43s\n","376:\tlearn: 0.1133913\ttotal: 4m 4s\tremaining: 6m 43s\n","377:\tlearn: 0.1133876\ttotal: 4m 4s\tremaining: 6m 42s\n","378:\tlearn: 0.1133190\ttotal: 4m 5s\tremaining: 6m 42s\n","379:\tlearn: 0.1131993\ttotal: 4m 6s\tremaining: 6m 41s\n","380:\tlearn: 0.1131762\ttotal: 4m 6s\tremaining: 6m 40s\n","381:\tlearn: 0.1126792\ttotal: 4m 7s\tremaining: 6m 40s\n","382:\tlearn: 0.1125434\ttotal: 4m 7s\tremaining: 6m 39s\n","383:\tlearn: 0.1123744\ttotal: 4m 8s\tremaining: 6m 38s\n","384:\tlearn: 0.1123192\ttotal: 4m 9s\tremaining: 6m 38s\n","385:\tlearn: 0.1120171\ttotal: 4m 9s\tremaining: 6m 37s\n","386:\tlearn: 0.1119074\ttotal: 4m 10s\tremaining: 6m 36s\n","387:\tlearn: 0.1118806\ttotal: 4m 11s\tremaining: 6m 36s\n","388:\tlearn: 0.1117840\ttotal: 4m 11s\tremaining: 6m 35s\n","389:\tlearn: 0.1116315\ttotal: 4m 12s\tremaining: 6m 34s\n","390:\tlearn: 0.1115626\ttotal: 4m 13s\tremaining: 6m 34s\n","391:\tlearn: 0.1115169\ttotal: 4m 13s\tremaining: 6m 33s\n","392:\tlearn: 0.1114956\ttotal: 4m 14s\tremaining: 6m 32s\n","393:\tlearn: 0.1114699\ttotal: 4m 14s\tremaining: 6m 32s\n","394:\tlearn: 0.1114250\ttotal: 4m 15s\tremaining: 6m 31s\n","395:\tlearn: 0.1112232\ttotal: 4m 16s\tremaining: 6m 30s\n","396:\tlearn: 0.1109459\ttotal: 4m 16s\tremaining: 6m 30s\n","397:\tlearn: 0.1109206\ttotal: 4m 17s\tremaining: 6m 29s\n","398:\tlearn: 0.1107863\ttotal: 4m 18s\tremaining: 6m 28s\n","399:\tlearn: 0.1105825\ttotal: 4m 18s\tremaining: 6m 28s\n","400:\tlearn: 0.1105013\ttotal: 4m 19s\tremaining: 6m 27s\n","401:\tlearn: 0.1103502\ttotal: 4m 20s\tremaining: 6m 26s\n","402:\tlearn: 0.1101820\ttotal: 4m 20s\tremaining: 6m 26s\n","403:\tlearn: 0.1101087\ttotal: 4m 21s\tremaining: 6m 25s\n","404:\tlearn: 0.1098815\ttotal: 4m 21s\tremaining: 6m 24s\n","405:\tlearn: 0.1098258\ttotal: 4m 22s\tremaining: 6m 24s\n","406:\tlearn: 0.1096549\ttotal: 4m 23s\tremaining: 6m 23s\n","407:\tlearn: 0.1096521\ttotal: 4m 23s\tremaining: 6m 22s\n","408:\tlearn: 0.1095767\ttotal: 4m 24s\tremaining: 6m 22s\n","409:\tlearn: 0.1094757\ttotal: 4m 25s\tremaining: 6m 21s\n","410:\tlearn: 0.1092157\ttotal: 4m 25s\tremaining: 6m 20s\n","411:\tlearn: 0.1091523\ttotal: 4m 26s\tremaining: 6m 20s\n","412:\tlearn: 0.1090554\ttotal: 4m 27s\tremaining: 6m 19s\n","413:\tlearn: 0.1089900\ttotal: 4m 27s\tremaining: 6m 18s\n","414:\tlearn: 0.1089070\ttotal: 4m 28s\tremaining: 6m 18s\n","415:\tlearn: 0.1088835\ttotal: 4m 28s\tremaining: 6m 17s\n","416:\tlearn: 0.1088384\ttotal: 4m 29s\tremaining: 6m 16s\n","417:\tlearn: 0.1085270\ttotal: 4m 30s\tremaining: 6m 16s\n","418:\tlearn: 0.1084826\ttotal: 4m 30s\tremaining: 6m 15s\n","419:\tlearn: 0.1084095\ttotal: 4m 31s\tremaining: 6m 14s\n","420:\tlearn: 0.1083979\ttotal: 4m 31s\tremaining: 6m 14s\n","421:\tlearn: 0.1083136\ttotal: 4m 32s\tremaining: 6m 13s\n","422:\tlearn: 0.1082365\ttotal: 4m 33s\tremaining: 6m 12s\n","423:\tlearn: 0.1080097\ttotal: 4m 33s\tremaining: 6m 11s\n","424:\tlearn: 0.1079532\ttotal: 4m 34s\tremaining: 6m 11s\n","425:\tlearn: 0.1078737\ttotal: 4m 35s\tremaining: 6m 10s\n","426:\tlearn: 0.1078722\ttotal: 4m 35s\tremaining: 6m 9s\n","427:\tlearn: 0.1078298\ttotal: 4m 36s\tremaining: 6m 9s\n","428:\tlearn: 0.1078134\ttotal: 4m 36s\tremaining: 6m 8s\n","429:\tlearn: 0.1077136\ttotal: 4m 37s\tremaining: 6m 7s\n","430:\tlearn: 0.1076407\ttotal: 4m 38s\tremaining: 6m 7s\n","431:\tlearn: 0.1075163\ttotal: 4m 38s\tremaining: 6m 6s\n","432:\tlearn: 0.1074585\ttotal: 4m 39s\tremaining: 6m 5s\n","433:\tlearn: 0.1074290\ttotal: 4m 39s\tremaining: 6m 5s\n","434:\tlearn: 0.1073995\ttotal: 4m 40s\tremaining: 6m 4s\n","435:\tlearn: 0.1073489\ttotal: 4m 41s\tremaining: 6m 3s\n","436:\tlearn: 0.1071984\ttotal: 4m 41s\tremaining: 6m 3s\n","437:\tlearn: 0.1070025\ttotal: 4m 42s\tremaining: 6m 2s\n","438:\tlearn: 0.1069465\ttotal: 4m 43s\tremaining: 6m 1s\n","439:\tlearn: 0.1068752\ttotal: 4m 43s\tremaining: 6m 1s\n","440:\tlearn: 0.1067528\ttotal: 4m 44s\tremaining: 6m\n","441:\tlearn: 0.1066429\ttotal: 4m 44s\tremaining: 5m 59s\n","442:\tlearn: 0.1064961\ttotal: 4m 45s\tremaining: 5m 59s\n","443:\tlearn: 0.1061989\ttotal: 4m 46s\tremaining: 5m 58s\n","444:\tlearn: 0.1061689\ttotal: 4m 46s\tremaining: 5m 57s\n","445:\tlearn: 0.1059710\ttotal: 4m 47s\tremaining: 5m 57s\n","446:\tlearn: 0.1057461\ttotal: 4m 48s\tremaining: 5m 56s\n","447:\tlearn: 0.1057186\ttotal: 4m 48s\tremaining: 5m 55s\n","448:\tlearn: 0.1056321\ttotal: 4m 49s\tremaining: 5m 55s\n","449:\tlearn: 0.1055797\ttotal: 4m 49s\tremaining: 5m 54s\n","450:\tlearn: 0.1055686\ttotal: 4m 50s\tremaining: 5m 53s\n","451:\tlearn: 0.1054483\ttotal: 4m 51s\tremaining: 5m 52s\n","452:\tlearn: 0.1053742\ttotal: 4m 51s\tremaining: 5m 52s\n","453:\tlearn: 0.1052609\ttotal: 4m 52s\tremaining: 5m 51s\n","454:\tlearn: 0.1051154\ttotal: 4m 53s\tremaining: 5m 50s\n","455:\tlearn: 0.1051062\ttotal: 4m 53s\tremaining: 5m 50s\n","456:\tlearn: 0.1050349\ttotal: 4m 54s\tremaining: 5m 49s\n","457:\tlearn: 0.1048604\ttotal: 4m 54s\tremaining: 5m 49s\n","458:\tlearn: 0.1047815\ttotal: 4m 55s\tremaining: 5m 48s\n","459:\tlearn: 0.1046846\ttotal: 4m 56s\tremaining: 5m 47s\n","460:\tlearn: 0.1045896\ttotal: 4m 56s\tremaining: 5m 46s\n","461:\tlearn: 0.1045733\ttotal: 4m 57s\tremaining: 5m 46s\n","462:\tlearn: 0.1045083\ttotal: 4m 57s\tremaining: 5m 45s\n","463:\tlearn: 0.1044180\ttotal: 4m 58s\tremaining: 5m 44s\n","464:\tlearn: 0.1042903\ttotal: 4m 59s\tremaining: 5m 44s\n","465:\tlearn: 0.1041695\ttotal: 4m 59s\tremaining: 5m 43s\n","466:\tlearn: 0.1041206\ttotal: 5m\tremaining: 5m 42s\n","467:\tlearn: 0.1041186\ttotal: 5m 1s\tremaining: 5m 42s\n","468:\tlearn: 0.1038663\ttotal: 5m 1s\tremaining: 5m 41s\n","469:\tlearn: 0.1036311\ttotal: 5m 2s\tremaining: 5m 40s\n","470:\tlearn: 0.1035903\ttotal: 5m 2s\tremaining: 5m 40s\n","471:\tlearn: 0.1035573\ttotal: 5m 3s\tremaining: 5m 39s\n","472:\tlearn: 0.1035548\ttotal: 5m 4s\tremaining: 5m 38s\n","473:\tlearn: 0.1035459\ttotal: 5m 4s\tremaining: 5m 38s\n","474:\tlearn: 0.1035123\ttotal: 5m 5s\tremaining: 5m 37s\n","475:\tlearn: 0.1034676\ttotal: 5m 5s\tremaining: 5m 36s\n","476:\tlearn: 0.1034422\ttotal: 5m 6s\tremaining: 5m 36s\n","477:\tlearn: 0.1034275\ttotal: 5m 7s\tremaining: 5m 35s\n","478:\tlearn: 0.1032579\ttotal: 5m 7s\tremaining: 5m 34s\n","479:\tlearn: 0.1029816\ttotal: 5m 8s\tremaining: 5m 34s\n","480:\tlearn: 0.1027966\ttotal: 5m 9s\tremaining: 5m 33s\n","481:\tlearn: 0.1027688\ttotal: 5m 9s\tremaining: 5m 32s\n","482:\tlearn: 0.1027445\ttotal: 5m 10s\tremaining: 5m 32s\n","483:\tlearn: 0.1026205\ttotal: 5m 10s\tremaining: 5m 31s\n","484:\tlearn: 0.1025038\ttotal: 5m 11s\tremaining: 5m 30s\n","485:\tlearn: 0.1022883\ttotal: 5m 12s\tremaining: 5m 30s\n","486:\tlearn: 0.1022733\ttotal: 5m 12s\tremaining: 5m 29s\n","487:\tlearn: 0.1022011\ttotal: 5m 13s\tremaining: 5m 28s\n","488:\tlearn: 0.1021985\ttotal: 5m 13s\tremaining: 5m 28s\n","489:\tlearn: 0.1020988\ttotal: 5m 14s\tremaining: 5m 27s\n","490:\tlearn: 0.1020069\ttotal: 5m 15s\tremaining: 5m 26s\n","491:\tlearn: 0.1018392\ttotal: 5m 15s\tremaining: 5m 26s\n","492:\tlearn: 0.1018178\ttotal: 5m 16s\tremaining: 5m 25s\n","493:\tlearn: 0.1018030\ttotal: 5m 17s\tremaining: 5m 24s\n","494:\tlearn: 0.1017597\ttotal: 5m 17s\tremaining: 5m 24s\n","495:\tlearn: 0.1017496\ttotal: 5m 18s\tremaining: 5m 23s\n","496:\tlearn: 0.1016356\ttotal: 5m 19s\tremaining: 5m 22s\n","497:\tlearn: 0.1016228\ttotal: 5m 19s\tremaining: 5m 22s\n","498:\tlearn: 0.1015665\ttotal: 5m 20s\tremaining: 5m 21s\n","499:\tlearn: 0.1014955\ttotal: 5m 20s\tremaining: 5m 20s\n","500:\tlearn: 0.1013750\ttotal: 5m 21s\tremaining: 5m 20s\n","501:\tlearn: 0.1013056\ttotal: 5m 22s\tremaining: 5m 19s\n","502:\tlearn: 0.1012495\ttotal: 5m 22s\tremaining: 5m 18s\n","503:\tlearn: 0.1011054\ttotal: 5m 23s\tremaining: 5m 18s\n","504:\tlearn: 0.1008990\ttotal: 5m 24s\tremaining: 5m 17s\n","505:\tlearn: 0.1007587\ttotal: 5m 24s\tremaining: 5m 16s\n","506:\tlearn: 0.1006313\ttotal: 5m 25s\tremaining: 5m 16s\n","507:\tlearn: 0.1005209\ttotal: 5m 25s\tremaining: 5m 15s\n","508:\tlearn: 0.1004264\ttotal: 5m 26s\tremaining: 5m 14s\n","509:\tlearn: 0.1004157\ttotal: 5m 27s\tremaining: 5m 14s\n","510:\tlearn: 0.1003724\ttotal: 5m 27s\tremaining: 5m 13s\n","511:\tlearn: 0.1003672\ttotal: 5m 28s\tremaining: 5m 12s\n","512:\tlearn: 0.1002826\ttotal: 5m 28s\tremaining: 5m 12s\n","513:\tlearn: 0.1002673\ttotal: 5m 29s\tremaining: 5m 11s\n","514:\tlearn: 0.1001193\ttotal: 5m 30s\tremaining: 5m 10s\n","515:\tlearn: 0.1000667\ttotal: 5m 30s\tremaining: 5m 10s\n","516:\tlearn: 0.0999623\ttotal: 5m 31s\tremaining: 5m 9s\n","517:\tlearn: 0.0998630\ttotal: 5m 31s\tremaining: 5m 8s\n","518:\tlearn: 0.0998132\ttotal: 5m 32s\tremaining: 5m 8s\n","519:\tlearn: 0.0995319\ttotal: 5m 33s\tremaining: 5m 7s\n","520:\tlearn: 0.0993753\ttotal: 5m 33s\tremaining: 5m 6s\n","521:\tlearn: 0.0993383\ttotal: 5m 34s\tremaining: 5m 6s\n","522:\tlearn: 0.0993282\ttotal: 5m 35s\tremaining: 5m 5s\n","523:\tlearn: 0.0992761\ttotal: 5m 35s\tremaining: 5m 4s\n","524:\tlearn: 0.0991808\ttotal: 5m 36s\tremaining: 5m 4s\n","525:\tlearn: 0.0990346\ttotal: 5m 36s\tremaining: 5m 3s\n","526:\tlearn: 0.0986910\ttotal: 5m 37s\tremaining: 5m 3s\n","527:\tlearn: 0.0986350\ttotal: 5m 38s\tremaining: 5m 2s\n","528:\tlearn: 0.0985853\ttotal: 5m 38s\tremaining: 5m 1s\n","529:\tlearn: 0.0985458\ttotal: 5m 39s\tremaining: 5m 1s\n","530:\tlearn: 0.0982162\ttotal: 5m 40s\tremaining: 5m\n","531:\tlearn: 0.0981482\ttotal: 5m 40s\tremaining: 4m 59s\n","532:\tlearn: 0.0978279\ttotal: 5m 41s\tremaining: 4m 59s\n","533:\tlearn: 0.0976107\ttotal: 5m 42s\tremaining: 4m 58s\n","534:\tlearn: 0.0975734\ttotal: 5m 42s\tremaining: 4m 57s\n","535:\tlearn: 0.0975262\ttotal: 5m 43s\tremaining: 4m 57s\n","536:\tlearn: 0.0974903\ttotal: 5m 43s\tremaining: 4m 56s\n","537:\tlearn: 0.0974536\ttotal: 5m 44s\tremaining: 4m 55s\n","538:\tlearn: 0.0974310\ttotal: 5m 45s\tremaining: 4m 55s\n","539:\tlearn: 0.0973699\ttotal: 5m 45s\tremaining: 4m 54s\n","540:\tlearn: 0.0972623\ttotal: 5m 46s\tremaining: 4m 53s\n","541:\tlearn: 0.0971058\ttotal: 5m 47s\tremaining: 4m 53s\n","542:\tlearn: 0.0970571\ttotal: 5m 47s\tremaining: 4m 52s\n","543:\tlearn: 0.0970181\ttotal: 5m 48s\tremaining: 4m 51s\n","544:\tlearn: 0.0969668\ttotal: 5m 48s\tremaining: 4m 51s\n","545:\tlearn: 0.0969317\ttotal: 5m 49s\tremaining: 4m 50s\n","546:\tlearn: 0.0968470\ttotal: 5m 50s\tremaining: 4m 49s\n","547:\tlearn: 0.0968138\ttotal: 5m 50s\tremaining: 4m 49s\n","548:\tlearn: 0.0968044\ttotal: 5m 51s\tremaining: 4m 48s\n","549:\tlearn: 0.0967369\ttotal: 5m 51s\tremaining: 4m 47s\n","550:\tlearn: 0.0967209\ttotal: 5m 52s\tremaining: 4m 47s\n","551:\tlearn: 0.0966603\ttotal: 5m 53s\tremaining: 4m 46s\n","552:\tlearn: 0.0966552\ttotal: 5m 53s\tremaining: 4m 46s\n","553:\tlearn: 0.0965861\ttotal: 5m 54s\tremaining: 4m 45s\n","554:\tlearn: 0.0965263\ttotal: 5m 55s\tremaining: 4m 44s\n","555:\tlearn: 0.0964536\ttotal: 5m 55s\tremaining: 4m 44s\n","556:\tlearn: 0.0964367\ttotal: 5m 56s\tremaining: 4m 43s\n","557:\tlearn: 0.0963512\ttotal: 5m 57s\tremaining: 4m 42s\n","558:\tlearn: 0.0963216\ttotal: 5m 57s\tremaining: 4m 42s\n","559:\tlearn: 0.0961736\ttotal: 5m 58s\tremaining: 4m 41s\n","560:\tlearn: 0.0961575\ttotal: 5m 58s\tremaining: 4m 40s\n","561:\tlearn: 0.0961150\ttotal: 5m 59s\tremaining: 4m 40s\n","562:\tlearn: 0.0958758\ttotal: 6m\tremaining: 4m 39s\n","563:\tlearn: 0.0957079\ttotal: 6m\tremaining: 4m 38s\n","564:\tlearn: 0.0956354\ttotal: 6m 1s\tremaining: 4m 38s\n","565:\tlearn: 0.0954998\ttotal: 6m 1s\tremaining: 4m 37s\n","566:\tlearn: 0.0954795\ttotal: 6m 2s\tremaining: 4m 36s\n","567:\tlearn: 0.0954395\ttotal: 6m 3s\tremaining: 4m 36s\n","568:\tlearn: 0.0953706\ttotal: 6m 3s\tremaining: 4m 35s\n","569:\tlearn: 0.0952656\ttotal: 6m 4s\tremaining: 4m 34s\n","570:\tlearn: 0.0950987\ttotal: 6m 5s\tremaining: 4m 34s\n","571:\tlearn: 0.0950515\ttotal: 6m 5s\tremaining: 4m 33s\n","572:\tlearn: 0.0949767\ttotal: 6m 6s\tremaining: 4m 32s\n","573:\tlearn: 0.0949257\ttotal: 6m 6s\tremaining: 4m 32s\n","574:\tlearn: 0.0947115\ttotal: 6m 7s\tremaining: 4m 31s\n","575:\tlearn: 0.0945758\ttotal: 6m 8s\tremaining: 4m 31s\n","576:\tlearn: 0.0945470\ttotal: 6m 8s\tremaining: 4m 30s\n","577:\tlearn: 0.0944709\ttotal: 6m 9s\tremaining: 4m 29s\n","578:\tlearn: 0.0944215\ttotal: 6m 10s\tremaining: 4m 29s\n","579:\tlearn: 0.0943541\ttotal: 6m 10s\tremaining: 4m 28s\n","580:\tlearn: 0.0942570\ttotal: 6m 11s\tremaining: 4m 27s\n","581:\tlearn: 0.0941929\ttotal: 6m 11s\tremaining: 4m 27s\n","582:\tlearn: 0.0941234\ttotal: 6m 12s\tremaining: 4m 26s\n","583:\tlearn: 0.0940365\ttotal: 6m 13s\tremaining: 4m 25s\n","584:\tlearn: 0.0939383\ttotal: 6m 13s\tremaining: 4m 25s\n","585:\tlearn: 0.0939246\ttotal: 6m 14s\tremaining: 4m 24s\n","586:\tlearn: 0.0938989\ttotal: 6m 15s\tremaining: 4m 24s\n","587:\tlearn: 0.0937824\ttotal: 6m 15s\tremaining: 4m 23s\n","588:\tlearn: 0.0937739\ttotal: 6m 16s\tremaining: 4m 22s\n","589:\tlearn: 0.0936884\ttotal: 6m 17s\tremaining: 4m 22s\n","590:\tlearn: 0.0936339\ttotal: 6m 17s\tremaining: 4m 21s\n","591:\tlearn: 0.0935814\ttotal: 6m 18s\tremaining: 4m 20s\n","592:\tlearn: 0.0934516\ttotal: 6m 19s\tremaining: 4m 20s\n","593:\tlearn: 0.0934259\ttotal: 6m 19s\tremaining: 4m 19s\n","594:\tlearn: 0.0933651\ttotal: 6m 20s\tremaining: 4m 18s\n","595:\tlearn: 0.0932574\ttotal: 6m 21s\tremaining: 4m 18s\n","596:\tlearn: 0.0931850\ttotal: 6m 21s\tremaining: 4m 17s\n","597:\tlearn: 0.0931392\ttotal: 6m 22s\tremaining: 4m 17s\n","598:\tlearn: 0.0929820\ttotal: 6m 23s\tremaining: 4m 16s\n","599:\tlearn: 0.0929500\ttotal: 6m 23s\tremaining: 4m 15s\n","600:\tlearn: 0.0928447\ttotal: 6m 24s\tremaining: 4m 15s\n","601:\tlearn: 0.0928109\ttotal: 6m 24s\tremaining: 4m 14s\n","602:\tlearn: 0.0927793\ttotal: 6m 25s\tremaining: 4m 13s\n","603:\tlearn: 0.0926978\ttotal: 6m 26s\tremaining: 4m 13s\n","604:\tlearn: 0.0924764\ttotal: 6m 26s\tremaining: 4m 12s\n","605:\tlearn: 0.0924539\ttotal: 6m 27s\tremaining: 4m 11s\n","606:\tlearn: 0.0922934\ttotal: 6m 28s\tremaining: 4m 11s\n","607:\tlearn: 0.0922514\ttotal: 6m 28s\tremaining: 4m 10s\n","608:\tlearn: 0.0922027\ttotal: 6m 29s\tremaining: 4m 10s\n","609:\tlearn: 0.0921886\ttotal: 6m 30s\tremaining: 4m 9s\n","610:\tlearn: 0.0921262\ttotal: 6m 30s\tremaining: 4m 8s\n","611:\tlearn: 0.0920530\ttotal: 6m 31s\tremaining: 4m 8s\n","612:\tlearn: 0.0920249\ttotal: 6m 31s\tremaining: 4m 7s\n","613:\tlearn: 0.0919998\ttotal: 6m 32s\tremaining: 4m 6s\n","614:\tlearn: 0.0919666\ttotal: 6m 33s\tremaining: 4m 6s\n","615:\tlearn: 0.0919556\ttotal: 6m 33s\tremaining: 4m 5s\n","616:\tlearn: 0.0918188\ttotal: 6m 34s\tremaining: 4m 4s\n","617:\tlearn: 0.0918082\ttotal: 6m 35s\tremaining: 4m 4s\n","618:\tlearn: 0.0918064\ttotal: 6m 35s\tremaining: 4m 3s\n","619:\tlearn: 0.0917121\ttotal: 6m 36s\tremaining: 4m 2s\n","620:\tlearn: 0.0917102\ttotal: 6m 37s\tremaining: 4m 2s\n","621:\tlearn: 0.0916662\ttotal: 6m 37s\tremaining: 4m 1s\n","622:\tlearn: 0.0915581\ttotal: 6m 38s\tremaining: 4m 1s\n","623:\tlearn: 0.0914970\ttotal: 6m 38s\tremaining: 4m\n","624:\tlearn: 0.0914378\ttotal: 6m 39s\tremaining: 3m 59s\n","625:\tlearn: 0.0914166\ttotal: 6m 40s\tremaining: 3m 59s\n","626:\tlearn: 0.0913577\ttotal: 6m 40s\tremaining: 3m 58s\n","627:\tlearn: 0.0913095\ttotal: 6m 41s\tremaining: 3m 57s\n","628:\tlearn: 0.0912136\ttotal: 6m 42s\tremaining: 3m 57s\n","629:\tlearn: 0.0911756\ttotal: 6m 42s\tremaining: 3m 56s\n","630:\tlearn: 0.0910390\ttotal: 6m 43s\tremaining: 3m 55s\n","631:\tlearn: 0.0907096\ttotal: 6m 44s\tremaining: 3m 55s\n","632:\tlearn: 0.0906876\ttotal: 6m 44s\tremaining: 3m 54s\n","633:\tlearn: 0.0906727\ttotal: 6m 45s\tremaining: 3m 53s\n","634:\tlearn: 0.0906630\ttotal: 6m 45s\tremaining: 3m 53s\n","635:\tlearn: 0.0906611\ttotal: 6m 46s\tremaining: 3m 52s\n","636:\tlearn: 0.0905939\ttotal: 6m 47s\tremaining: 3m 52s\n","637:\tlearn: 0.0905828\ttotal: 6m 47s\tremaining: 3m 51s\n","638:\tlearn: 0.0905682\ttotal: 6m 48s\tremaining: 3m 50s\n","639:\tlearn: 0.0905272\ttotal: 6m 49s\tremaining: 3m 50s\n","640:\tlearn: 0.0904432\ttotal: 6m 49s\tremaining: 3m 49s\n","641:\tlearn: 0.0904154\ttotal: 6m 50s\tremaining: 3m 48s\n","642:\tlearn: 0.0903874\ttotal: 6m 51s\tremaining: 3m 48s\n","643:\tlearn: 0.0903298\ttotal: 6m 51s\tremaining: 3m 47s\n","644:\tlearn: 0.0902572\ttotal: 6m 52s\tremaining: 3m 46s\n","645:\tlearn: 0.0902535\ttotal: 6m 53s\tremaining: 3m 46s\n","646:\tlearn: 0.0902125\ttotal: 6m 53s\tremaining: 3m 45s\n","647:\tlearn: 0.0900824\ttotal: 6m 54s\tremaining: 3m 45s\n","648:\tlearn: 0.0900158\ttotal: 6m 54s\tremaining: 3m 44s\n","649:\tlearn: 0.0899316\ttotal: 6m 55s\tremaining: 3m 43s\n","650:\tlearn: 0.0898381\ttotal: 6m 56s\tremaining: 3m 43s\n","651:\tlearn: 0.0897900\ttotal: 6m 56s\tremaining: 3m 42s\n","652:\tlearn: 0.0895852\ttotal: 6m 57s\tremaining: 3m 41s\n","653:\tlearn: 0.0895578\ttotal: 6m 58s\tremaining: 3m 41s\n","654:\tlearn: 0.0895494\ttotal: 6m 58s\tremaining: 3m 40s\n","655:\tlearn: 0.0894923\ttotal: 6m 59s\tremaining: 3m 39s\n","656:\tlearn: 0.0893612\ttotal: 7m\tremaining: 3m 39s\n","657:\tlearn: 0.0893304\ttotal: 7m\tremaining: 3m 38s\n","658:\tlearn: 0.0893285\ttotal: 7m 1s\tremaining: 3m 38s\n","659:\tlearn: 0.0892170\ttotal: 7m 2s\tremaining: 3m 37s\n","660:\tlearn: 0.0891617\ttotal: 7m 2s\tremaining: 3m 36s\n","661:\tlearn: 0.0891108\ttotal: 7m 3s\tremaining: 3m 36s\n","662:\tlearn: 0.0890946\ttotal: 7m 3s\tremaining: 3m 35s\n","663:\tlearn: 0.0889949\ttotal: 7m 4s\tremaining: 3m 34s\n","664:\tlearn: 0.0887965\ttotal: 7m 5s\tremaining: 3m 34s\n","665:\tlearn: 0.0887055\ttotal: 7m 5s\tremaining: 3m 33s\n","666:\tlearn: 0.0886724\ttotal: 7m 6s\tremaining: 3m 32s\n","667:\tlearn: 0.0886389\ttotal: 7m 7s\tremaining: 3m 32s\n","668:\tlearn: 0.0884790\ttotal: 7m 7s\tremaining: 3m 31s\n","669:\tlearn: 0.0884228\ttotal: 7m 8s\tremaining: 3m 31s\n","670:\tlearn: 0.0883862\ttotal: 7m 9s\tremaining: 3m 30s\n","671:\tlearn: 0.0882639\ttotal: 7m 9s\tremaining: 3m 29s\n","672:\tlearn: 0.0882438\ttotal: 7m 10s\tremaining: 3m 29s\n","673:\tlearn: 0.0882276\ttotal: 7m 11s\tremaining: 3m 28s\n","674:\tlearn: 0.0881975\ttotal: 7m 11s\tremaining: 3m 27s\n","675:\tlearn: 0.0880128\ttotal: 7m 12s\tremaining: 3m 27s\n","676:\tlearn: 0.0879965\ttotal: 7m 12s\tremaining: 3m 26s\n","677:\tlearn: 0.0879861\ttotal: 7m 13s\tremaining: 3m 25s\n","678:\tlearn: 0.0878224\ttotal: 7m 14s\tremaining: 3m 25s\n","679:\tlearn: 0.0876672\ttotal: 7m 14s\tremaining: 3m 24s\n","680:\tlearn: 0.0876237\ttotal: 7m 15s\tremaining: 3m 23s\n","681:\tlearn: 0.0875733\ttotal: 7m 16s\tremaining: 3m 23s\n","682:\tlearn: 0.0874638\ttotal: 7m 16s\tremaining: 3m 22s\n","683:\tlearn: 0.0874171\ttotal: 7m 17s\tremaining: 3m 22s\n","684:\tlearn: 0.0874136\ttotal: 7m 17s\tremaining: 3m 21s\n","685:\tlearn: 0.0874012\ttotal: 7m 18s\tremaining: 3m 20s\n","686:\tlearn: 0.0873847\ttotal: 7m 19s\tremaining: 3m 20s\n","687:\tlearn: 0.0873474\ttotal: 7m 19s\tremaining: 3m 19s\n","688:\tlearn: 0.0872732\ttotal: 7m 20s\tremaining: 3m 18s\n","689:\tlearn: 0.0871351\ttotal: 7m 20s\tremaining: 3m 18s\n","690:\tlearn: 0.0870317\ttotal: 7m 21s\tremaining: 3m 17s\n","691:\tlearn: 0.0869807\ttotal: 7m 22s\tremaining: 3m 16s\n","692:\tlearn: 0.0869639\ttotal: 7m 22s\tremaining: 3m 16s\n","693:\tlearn: 0.0868072\ttotal: 7m 23s\tremaining: 3m 15s\n","694:\tlearn: 0.0867851\ttotal: 7m 24s\tremaining: 3m 14s\n","695:\tlearn: 0.0867020\ttotal: 7m 24s\tremaining: 3m 14s\n","696:\tlearn: 0.0865929\ttotal: 7m 25s\tremaining: 3m 13s\n","697:\tlearn: 0.0865388\ttotal: 7m 26s\tremaining: 3m 12s\n","698:\tlearn: 0.0862255\ttotal: 7m 26s\tremaining: 3m 12s\n","699:\tlearn: 0.0861755\ttotal: 7m 27s\tremaining: 3m 11s\n","700:\tlearn: 0.0859910\ttotal: 7m 27s\tremaining: 3m 11s\n","701:\tlearn: 0.0858622\ttotal: 7m 28s\tremaining: 3m 10s\n","702:\tlearn: 0.0858533\ttotal: 7m 29s\tremaining: 3m 9s\n","703:\tlearn: 0.0858211\ttotal: 7m 29s\tremaining: 3m 9s\n","704:\tlearn: 0.0857809\ttotal: 7m 30s\tremaining: 3m 8s\n","705:\tlearn: 0.0857497\ttotal: 7m 31s\tremaining: 3m 7s\n","706:\tlearn: 0.0856691\ttotal: 7m 31s\tremaining: 3m 7s\n","707:\tlearn: 0.0856035\ttotal: 7m 32s\tremaining: 3m 6s\n","708:\tlearn: 0.0855273\ttotal: 7m 32s\tremaining: 3m 5s\n","709:\tlearn: 0.0853421\ttotal: 7m 33s\tremaining: 3m 5s\n","710:\tlearn: 0.0852936\ttotal: 7m 34s\tremaining: 3m 4s\n","711:\tlearn: 0.0851570\ttotal: 7m 34s\tremaining: 3m 3s\n","712:\tlearn: 0.0851552\ttotal: 7m 35s\tremaining: 3m 3s\n","713:\tlearn: 0.0850619\ttotal: 7m 36s\tremaining: 3m 2s\n","714:\tlearn: 0.0850332\ttotal: 7m 36s\tremaining: 3m 2s\n","715:\tlearn: 0.0849700\ttotal: 7m 37s\tremaining: 3m 1s\n","716:\tlearn: 0.0849307\ttotal: 7m 37s\tremaining: 3m\n","717:\tlearn: 0.0848566\ttotal: 7m 38s\tremaining: 3m\n","718:\tlearn: 0.0848425\ttotal: 7m 39s\tremaining: 2m 59s\n","719:\tlearn: 0.0847562\ttotal: 7m 39s\tremaining: 2m 58s\n","720:\tlearn: 0.0847022\ttotal: 7m 40s\tremaining: 2m 58s\n","721:\tlearn: 0.0846931\ttotal: 7m 41s\tremaining: 2m 57s\n","722:\tlearn: 0.0846490\ttotal: 7m 41s\tremaining: 2m 56s\n","723:\tlearn: 0.0845999\ttotal: 7m 42s\tremaining: 2m 56s\n","724:\tlearn: 0.0845424\ttotal: 7m 42s\tremaining: 2m 55s\n","725:\tlearn: 0.0844374\ttotal: 7m 43s\tremaining: 2m 54s\n","726:\tlearn: 0.0844172\ttotal: 7m 44s\tremaining: 2m 54s\n","727:\tlearn: 0.0843822\ttotal: 7m 44s\tremaining: 2m 53s\n","728:\tlearn: 0.0843122\ttotal: 7m 45s\tremaining: 2m 53s\n","729:\tlearn: 0.0842884\ttotal: 7m 46s\tremaining: 2m 52s\n","730:\tlearn: 0.0842347\ttotal: 7m 46s\tremaining: 2m 51s\n","731:\tlearn: 0.0842274\ttotal: 7m 47s\tremaining: 2m 51s\n","732:\tlearn: 0.0841041\ttotal: 7m 47s\tremaining: 2m 50s\n","733:\tlearn: 0.0839705\ttotal: 7m 48s\tremaining: 2m 49s\n","734:\tlearn: 0.0838319\ttotal: 7m 49s\tremaining: 2m 49s\n","735:\tlearn: 0.0837778\ttotal: 7m 49s\tremaining: 2m 48s\n","736:\tlearn: 0.0836081\ttotal: 7m 50s\tremaining: 2m 47s\n","737:\tlearn: 0.0834066\ttotal: 7m 51s\tremaining: 2m 47s\n","738:\tlearn: 0.0833749\ttotal: 7m 51s\tremaining: 2m 46s\n","739:\tlearn: 0.0833380\ttotal: 7m 52s\tremaining: 2m 45s\n","740:\tlearn: 0.0832486\ttotal: 7m 53s\tremaining: 2m 45s\n","741:\tlearn: 0.0831763\ttotal: 7m 53s\tremaining: 2m 44s\n","742:\tlearn: 0.0831465\ttotal: 7m 54s\tremaining: 2m 44s\n","743:\tlearn: 0.0831010\ttotal: 7m 54s\tremaining: 2m 43s\n","744:\tlearn: 0.0830435\ttotal: 7m 55s\tremaining: 2m 42s\n","745:\tlearn: 0.0829911\ttotal: 7m 56s\tremaining: 2m 42s\n","746:\tlearn: 0.0829863\ttotal: 7m 56s\tremaining: 2m 41s\n","747:\tlearn: 0.0829456\ttotal: 7m 57s\tremaining: 2m 40s\n","748:\tlearn: 0.0829049\ttotal: 7m 57s\tremaining: 2m 40s\n","749:\tlearn: 0.0827400\ttotal: 7m 58s\tremaining: 2m 39s\n","750:\tlearn: 0.0827170\ttotal: 7m 59s\tremaining: 2m 38s\n","751:\tlearn: 0.0826216\ttotal: 7m 59s\tremaining: 2m 38s\n","752:\tlearn: 0.0825757\ttotal: 8m\tremaining: 2m 37s\n","753:\tlearn: 0.0825499\ttotal: 8m 1s\tremaining: 2m 36s\n","754:\tlearn: 0.0825168\ttotal: 8m 1s\tremaining: 2m 36s\n","755:\tlearn: 0.0824198\ttotal: 8m 2s\tremaining: 2m 35s\n","756:\tlearn: 0.0823626\ttotal: 8m 2s\tremaining: 2m 35s\n","757:\tlearn: 0.0823084\ttotal: 8m 3s\tremaining: 2m 34s\n","758:\tlearn: 0.0822945\ttotal: 8m 4s\tremaining: 2m 33s\n","759:\tlearn: 0.0822356\ttotal: 8m 4s\tremaining: 2m 33s\n","760:\tlearn: 0.0822272\ttotal: 8m 5s\tremaining: 2m 32s\n","761:\tlearn: 0.0821905\ttotal: 8m 5s\tremaining: 2m 31s\n","762:\tlearn: 0.0821585\ttotal: 8m 6s\tremaining: 2m 31s\n","763:\tlearn: 0.0820556\ttotal: 8m 7s\tremaining: 2m 30s\n","764:\tlearn: 0.0819919\ttotal: 8m 7s\tremaining: 2m 29s\n","765:\tlearn: 0.0819245\ttotal: 8m 8s\tremaining: 2m 29s\n","766:\tlearn: 0.0818998\ttotal: 8m 9s\tremaining: 2m 28s\n","767:\tlearn: 0.0818310\ttotal: 8m 9s\tremaining: 2m 27s\n","768:\tlearn: 0.0818050\ttotal: 8m 10s\tremaining: 2m 27s\n","769:\tlearn: 0.0817535\ttotal: 8m 10s\tremaining: 2m 26s\n","770:\tlearn: 0.0817205\ttotal: 8m 11s\tremaining: 2m 25s\n","771:\tlearn: 0.0817033\ttotal: 8m 12s\tremaining: 2m 25s\n","772:\tlearn: 0.0816732\ttotal: 8m 12s\tremaining: 2m 24s\n","773:\tlearn: 0.0816716\ttotal: 8m 13s\tremaining: 2m 24s\n","774:\tlearn: 0.0816338\ttotal: 8m 13s\tremaining: 2m 23s\n","775:\tlearn: 0.0815544\ttotal: 8m 14s\tremaining: 2m 22s\n","776:\tlearn: 0.0815107\ttotal: 8m 15s\tremaining: 2m 22s\n","777:\tlearn: 0.0814746\ttotal: 8m 15s\tremaining: 2m 21s\n","778:\tlearn: 0.0814471\ttotal: 8m 16s\tremaining: 2m 20s\n","779:\tlearn: 0.0814261\ttotal: 8m 17s\tremaining: 2m 20s\n","780:\tlearn: 0.0813944\ttotal: 8m 17s\tremaining: 2m 19s\n","781:\tlearn: 0.0813769\ttotal: 8m 18s\tremaining: 2m 18s\n","782:\tlearn: 0.0813069\ttotal: 8m 19s\tremaining: 2m 18s\n","783:\tlearn: 0.0812578\ttotal: 8m 19s\tremaining: 2m 17s\n","784:\tlearn: 0.0812371\ttotal: 8m 20s\tremaining: 2m 17s\n","785:\tlearn: 0.0812027\ttotal: 8m 20s\tremaining: 2m 16s\n","786:\tlearn: 0.0811068\ttotal: 8m 21s\tremaining: 2m 15s\n","787:\tlearn: 0.0811037\ttotal: 8m 22s\tremaining: 2m 15s\n","788:\tlearn: 0.0809853\ttotal: 8m 22s\tremaining: 2m 14s\n","789:\tlearn: 0.0808413\ttotal: 8m 23s\tremaining: 2m 13s\n","790:\tlearn: 0.0806213\ttotal: 8m 24s\tremaining: 2m 13s\n","791:\tlearn: 0.0805457\ttotal: 8m 24s\tremaining: 2m 12s\n","792:\tlearn: 0.0805200\ttotal: 8m 25s\tremaining: 2m 11s\n","793:\tlearn: 0.0804756\ttotal: 8m 26s\tremaining: 2m 11s\n","794:\tlearn: 0.0803955\ttotal: 8m 26s\tremaining: 2m 10s\n","795:\tlearn: 0.0803618\ttotal: 8m 27s\tremaining: 2m 10s\n","796:\tlearn: 0.0803209\ttotal: 8m 28s\tremaining: 2m 9s\n","797:\tlearn: 0.0801879\ttotal: 8m 28s\tremaining: 2m 8s\n","798:\tlearn: 0.0801830\ttotal: 8m 29s\tremaining: 2m 8s\n","799:\tlearn: 0.0801241\ttotal: 8m 29s\tremaining: 2m 7s\n","800:\tlearn: 0.0800180\ttotal: 8m 30s\tremaining: 2m 6s\n","801:\tlearn: 0.0798647\ttotal: 8m 31s\tremaining: 2m 6s\n","802:\tlearn: 0.0798323\ttotal: 8m 31s\tremaining: 2m 5s\n","803:\tlearn: 0.0797686\ttotal: 8m 32s\tremaining: 2m 4s\n","804:\tlearn: 0.0797216\ttotal: 8m 33s\tremaining: 2m 4s\n","805:\tlearn: 0.0797115\ttotal: 8m 33s\tremaining: 2m 3s\n","806:\tlearn: 0.0796985\ttotal: 8m 34s\tremaining: 2m 3s\n","807:\tlearn: 0.0796015\ttotal: 8m 35s\tremaining: 2m 2s\n","808:\tlearn: 0.0795899\ttotal: 8m 35s\tremaining: 2m 1s\n","809:\tlearn: 0.0794034\ttotal: 8m 36s\tremaining: 2m 1s\n","810:\tlearn: 0.0793564\ttotal: 8m 37s\tremaining: 2m\n","811:\tlearn: 0.0793398\ttotal: 8m 37s\tremaining: 1m 59s\n","812:\tlearn: 0.0792369\ttotal: 8m 38s\tremaining: 1m 59s\n","813:\tlearn: 0.0792043\ttotal: 8m 39s\tremaining: 1m 58s\n","814:\tlearn: 0.0791063\ttotal: 8m 39s\tremaining: 1m 57s\n","815:\tlearn: 0.0790187\ttotal: 8m 40s\tremaining: 1m 57s\n","816:\tlearn: 0.0789873\ttotal: 8m 40s\tremaining: 1m 56s\n","817:\tlearn: 0.0789437\ttotal: 8m 41s\tremaining: 1m 56s\n","818:\tlearn: 0.0789320\ttotal: 8m 42s\tremaining: 1m 55s\n","819:\tlearn: 0.0788514\ttotal: 8m 42s\tremaining: 1m 54s\n","820:\tlearn: 0.0787996\ttotal: 8m 43s\tremaining: 1m 54s\n","821:\tlearn: 0.0786668\ttotal: 8m 44s\tremaining: 1m 53s\n","822:\tlearn: 0.0785221\ttotal: 8m 44s\tremaining: 1m 52s\n","823:\tlearn: 0.0783878\ttotal: 8m 45s\tremaining: 1m 52s\n","824:\tlearn: 0.0783083\ttotal: 8m 46s\tremaining: 1m 51s\n","825:\tlearn: 0.0782572\ttotal: 8m 46s\tremaining: 1m 50s\n","826:\tlearn: 0.0782425\ttotal: 8m 47s\tremaining: 1m 50s\n","827:\tlearn: 0.0781357\ttotal: 8m 48s\tremaining: 1m 49s\n","828:\tlearn: 0.0780869\ttotal: 8m 48s\tremaining: 1m 49s\n","829:\tlearn: 0.0780305\ttotal: 8m 49s\tremaining: 1m 48s\n","830:\tlearn: 0.0779858\ttotal: 8m 50s\tremaining: 1m 47s\n","831:\tlearn: 0.0779411\ttotal: 8m 50s\tremaining: 1m 47s\n","832:\tlearn: 0.0779086\ttotal: 8m 51s\tremaining: 1m 46s\n","833:\tlearn: 0.0778795\ttotal: 8m 52s\tremaining: 1m 45s\n","834:\tlearn: 0.0778377\ttotal: 8m 52s\tremaining: 1m 45s\n","835:\tlearn: 0.0777971\ttotal: 8m 53s\tremaining: 1m 44s\n","836:\tlearn: 0.0776995\ttotal: 8m 53s\tremaining: 1m 43s\n","837:\tlearn: 0.0776521\ttotal: 8m 54s\tremaining: 1m 43s\n","838:\tlearn: 0.0776189\ttotal: 8m 55s\tremaining: 1m 42s\n","839:\tlearn: 0.0775431\ttotal: 8m 55s\tremaining: 1m 42s\n","840:\tlearn: 0.0775249\ttotal: 8m 56s\tremaining: 1m 41s\n","841:\tlearn: 0.0772852\ttotal: 8m 57s\tremaining: 1m 40s\n","842:\tlearn: 0.0771273\ttotal: 8m 57s\tremaining: 1m 40s\n","843:\tlearn: 0.0770551\ttotal: 8m 58s\tremaining: 1m 39s\n","844:\tlearn: 0.0770237\ttotal: 8m 59s\tremaining: 1m 38s\n","845:\tlearn: 0.0770121\ttotal: 8m 59s\tremaining: 1m 38s\n","846:\tlearn: 0.0770010\ttotal: 9m\tremaining: 1m 37s\n","847:\tlearn: 0.0769738\ttotal: 9m\tremaining: 1m 36s\n","848:\tlearn: 0.0769361\ttotal: 9m 1s\tremaining: 1m 36s\n","849:\tlearn: 0.0769101\ttotal: 9m 2s\tremaining: 1m 35s\n","850:\tlearn: 0.0769040\ttotal: 9m 2s\tremaining: 1m 35s\n","851:\tlearn: 0.0768900\ttotal: 9m 3s\tremaining: 1m 34s\n","852:\tlearn: 0.0768449\ttotal: 9m 4s\tremaining: 1m 33s\n","853:\tlearn: 0.0768241\ttotal: 9m 4s\tremaining: 1m 33s\n","854:\tlearn: 0.0768138\ttotal: 9m 5s\tremaining: 1m 32s\n","855:\tlearn: 0.0767697\ttotal: 9m 6s\tremaining: 1m 31s\n","856:\tlearn: 0.0766174\ttotal: 9m 6s\tremaining: 1m 31s\n","857:\tlearn: 0.0766023\ttotal: 9m 7s\tremaining: 1m 30s\n","858:\tlearn: 0.0765707\ttotal: 9m 7s\tremaining: 1m 29s\n","859:\tlearn: 0.0765144\ttotal: 9m 8s\tremaining: 1m 29s\n","860:\tlearn: 0.0765081\ttotal: 9m 9s\tremaining: 1m 28s\n","861:\tlearn: 0.0765025\ttotal: 9m 9s\tremaining: 1m 28s\n","862:\tlearn: 0.0764632\ttotal: 9m 10s\tremaining: 1m 27s\n","863:\tlearn: 0.0764028\ttotal: 9m 11s\tremaining: 1m 26s\n","864:\tlearn: 0.0763891\ttotal: 9m 11s\tremaining: 1m 26s\n","865:\tlearn: 0.0763679\ttotal: 9m 12s\tremaining: 1m 25s\n","866:\tlearn: 0.0761999\ttotal: 9m 13s\tremaining: 1m 24s\n","867:\tlearn: 0.0761299\ttotal: 9m 13s\tremaining: 1m 24s\n","868:\tlearn: 0.0760004\ttotal: 9m 14s\tremaining: 1m 23s\n","869:\tlearn: 0.0759408\ttotal: 9m 15s\tremaining: 1m 22s\n","870:\tlearn: 0.0759303\ttotal: 9m 15s\tremaining: 1m 22s\n","871:\tlearn: 0.0758488\ttotal: 9m 16s\tremaining: 1m 21s\n","872:\tlearn: 0.0758477\ttotal: 9m 16s\tremaining: 1m 21s\n","873:\tlearn: 0.0757138\ttotal: 9m 17s\tremaining: 1m 20s\n","874:\tlearn: 0.0756880\ttotal: 9m 18s\tremaining: 1m 19s\n","875:\tlearn: 0.0756026\ttotal: 9m 18s\tremaining: 1m 19s\n","876:\tlearn: 0.0755976\ttotal: 9m 19s\tremaining: 1m 18s\n","877:\tlearn: 0.0755746\ttotal: 9m 20s\tremaining: 1m 17s\n","878:\tlearn: 0.0755174\ttotal: 9m 20s\tremaining: 1m 17s\n","879:\tlearn: 0.0754294\ttotal: 9m 21s\tremaining: 1m 16s\n","880:\tlearn: 0.0754204\ttotal: 9m 22s\tremaining: 1m 15s\n","881:\tlearn: 0.0753531\ttotal: 9m 22s\tremaining: 1m 15s\n","882:\tlearn: 0.0753137\ttotal: 9m 23s\tremaining: 1m 14s\n","883:\tlearn: 0.0751743\ttotal: 9m 24s\tremaining: 1m 14s\n","884:\tlearn: 0.0751184\ttotal: 9m 24s\tremaining: 1m 13s\n","885:\tlearn: 0.0750955\ttotal: 9m 25s\tremaining: 1m 12s\n","886:\tlearn: 0.0750767\ttotal: 9m 25s\tremaining: 1m 12s\n","887:\tlearn: 0.0750007\ttotal: 9m 26s\tremaining: 1m 11s\n","888:\tlearn: 0.0749391\ttotal: 9m 27s\tremaining: 1m 10s\n","889:\tlearn: 0.0749255\ttotal: 9m 27s\tremaining: 1m 10s\n","890:\tlearn: 0.0749047\ttotal: 9m 28s\tremaining: 1m 9s\n","891:\tlearn: 0.0748681\ttotal: 9m 29s\tremaining: 1m 8s\n","892:\tlearn: 0.0748104\ttotal: 9m 29s\tremaining: 1m 8s\n","893:\tlearn: 0.0747843\ttotal: 9m 30s\tremaining: 1m 7s\n","894:\tlearn: 0.0747815\ttotal: 9m 31s\tremaining: 1m 7s\n","895:\tlearn: 0.0746722\ttotal: 9m 31s\tremaining: 1m 6s\n","896:\tlearn: 0.0745772\ttotal: 9m 32s\tremaining: 1m 5s\n","897:\tlearn: 0.0745632\ttotal: 9m 32s\tremaining: 1m 5s\n","898:\tlearn: 0.0744773\ttotal: 9m 33s\tremaining: 1m 4s\n","899:\tlearn: 0.0744424\ttotal: 9m 34s\tremaining: 1m 3s\n","900:\tlearn: 0.0743761\ttotal: 9m 34s\tremaining: 1m 3s\n","901:\tlearn: 0.0742796\ttotal: 9m 35s\tremaining: 1m 2s\n","902:\tlearn: 0.0741446\ttotal: 9m 36s\tremaining: 1m 1s\n","903:\tlearn: 0.0741190\ttotal: 9m 36s\tremaining: 1m 1s\n","904:\tlearn: 0.0740851\ttotal: 9m 37s\tremaining: 1m\n","905:\tlearn: 0.0739770\ttotal: 9m 38s\tremaining: 60s\n","906:\tlearn: 0.0739674\ttotal: 9m 38s\tremaining: 59.3s\n","907:\tlearn: 0.0739143\ttotal: 9m 39s\tremaining: 58.7s\n","908:\tlearn: 0.0739102\ttotal: 9m 39s\tremaining: 58.1s\n","909:\tlearn: 0.0739019\ttotal: 9m 40s\tremaining: 57.4s\n","910:\tlearn: 0.0738448\ttotal: 9m 41s\tremaining: 56.8s\n","911:\tlearn: 0.0738404\ttotal: 9m 41s\tremaining: 56.1s\n","912:\tlearn: 0.0738229\ttotal: 9m 42s\tremaining: 55.5s\n","913:\tlearn: 0.0737964\ttotal: 9m 42s\tremaining: 54.8s\n","914:\tlearn: 0.0737635\ttotal: 9m 43s\tremaining: 54.2s\n","915:\tlearn: 0.0737434\ttotal: 9m 44s\tremaining: 53.6s\n","916:\tlearn: 0.0736935\ttotal: 9m 44s\tremaining: 52.9s\n","917:\tlearn: 0.0735993\ttotal: 9m 45s\tremaining: 52.3s\n","918:\tlearn: 0.0733905\ttotal: 9m 46s\tremaining: 51.7s\n","919:\tlearn: 0.0733620\ttotal: 9m 46s\tremaining: 51s\n","920:\tlearn: 0.0733485\ttotal: 9m 47s\tremaining: 50.4s\n","921:\tlearn: 0.0733254\ttotal: 9m 47s\tremaining: 49.7s\n","922:\tlearn: 0.0732953\ttotal: 9m 48s\tremaining: 49.1s\n","923:\tlearn: 0.0732742\ttotal: 9m 49s\tremaining: 48.5s\n","924:\tlearn: 0.0731914\ttotal: 9m 49s\tremaining: 47.8s\n","925:\tlearn: 0.0730851\ttotal: 9m 50s\tremaining: 47.2s\n","926:\tlearn: 0.0730315\ttotal: 9m 50s\tremaining: 46.5s\n","927:\tlearn: 0.0730026\ttotal: 9m 51s\tremaining: 45.9s\n","928:\tlearn: 0.0729815\ttotal: 9m 52s\tremaining: 45.3s\n","929:\tlearn: 0.0729423\ttotal: 9m 52s\tremaining: 44.6s\n","930:\tlearn: 0.0728970\ttotal: 9m 53s\tremaining: 44s\n","931:\tlearn: 0.0728307\ttotal: 9m 54s\tremaining: 43.3s\n","932:\tlearn: 0.0728218\ttotal: 9m 54s\tremaining: 42.7s\n","933:\tlearn: 0.0727541\ttotal: 9m 55s\tremaining: 42.1s\n","934:\tlearn: 0.0727092\ttotal: 9m 55s\tremaining: 41.4s\n","935:\tlearn: 0.0726592\ttotal: 9m 56s\tremaining: 40.8s\n","936:\tlearn: 0.0726129\ttotal: 9m 57s\tremaining: 40.2s\n","937:\tlearn: 0.0725980\ttotal: 9m 57s\tremaining: 39.5s\n","938:\tlearn: 0.0725756\ttotal: 9m 58s\tremaining: 38.9s\n","939:\tlearn: 0.0725686\ttotal: 9m 59s\tremaining: 38.2s\n","940:\tlearn: 0.0725128\ttotal: 9m 59s\tremaining: 37.6s\n","941:\tlearn: 0.0724243\ttotal: 10m\tremaining: 37s\n","942:\tlearn: 0.0724117\ttotal: 10m\tremaining: 36.3s\n","943:\tlearn: 0.0723428\ttotal: 10m 1s\tremaining: 35.7s\n","944:\tlearn: 0.0722779\ttotal: 10m 2s\tremaining: 35s\n","945:\tlearn: 0.0722408\ttotal: 10m 2s\tremaining: 34.4s\n","946:\tlearn: 0.0721854\ttotal: 10m 3s\tremaining: 33.8s\n","947:\tlearn: 0.0720946\ttotal: 10m 4s\tremaining: 33.1s\n","948:\tlearn: 0.0720224\ttotal: 10m 4s\tremaining: 32.5s\n","949:\tlearn: 0.0720166\ttotal: 10m 5s\tremaining: 31.9s\n","950:\tlearn: 0.0720109\ttotal: 10m 5s\tremaining: 31.2s\n","951:\tlearn: 0.0719519\ttotal: 10m 6s\tremaining: 30.6s\n","952:\tlearn: 0.0718699\ttotal: 10m 7s\tremaining: 29.9s\n","953:\tlearn: 0.0718212\ttotal: 10m 7s\tremaining: 29.3s\n","954:\tlearn: 0.0717532\ttotal: 10m 8s\tremaining: 28.7s\n","955:\tlearn: 0.0717291\ttotal: 10m 9s\tremaining: 28s\n","956:\tlearn: 0.0717050\ttotal: 10m 9s\tremaining: 27.4s\n","957:\tlearn: 0.0716741\ttotal: 10m 10s\tremaining: 26.8s\n","958:\tlearn: 0.0716400\ttotal: 10m 10s\tremaining: 26.1s\n","959:\tlearn: 0.0715824\ttotal: 10m 11s\tremaining: 25.5s\n","960:\tlearn: 0.0715710\ttotal: 10m 12s\tremaining: 24.8s\n","961:\tlearn: 0.0715398\ttotal: 10m 12s\tremaining: 24.2s\n","962:\tlearn: 0.0715326\ttotal: 10m 13s\tremaining: 23.6s\n","963:\tlearn: 0.0713529\ttotal: 10m 14s\tremaining: 22.9s\n","964:\tlearn: 0.0713080\ttotal: 10m 14s\tremaining: 22.3s\n","965:\tlearn: 0.0711821\ttotal: 10m 15s\tremaining: 21.7s\n","966:\tlearn: 0.0711302\ttotal: 10m 15s\tremaining: 21s\n","967:\tlearn: 0.0711107\ttotal: 10m 16s\tremaining: 20.4s\n","968:\tlearn: 0.0710283\ttotal: 10m 17s\tremaining: 19.7s\n","969:\tlearn: 0.0709060\ttotal: 10m 17s\tremaining: 19.1s\n","970:\tlearn: 0.0708737\ttotal: 10m 18s\tremaining: 18.5s\n","971:\tlearn: 0.0708654\ttotal: 10m 19s\tremaining: 17.8s\n","972:\tlearn: 0.0708169\ttotal: 10m 19s\tremaining: 17.2s\n","973:\tlearn: 0.0707827\ttotal: 10m 20s\tremaining: 16.6s\n","974:\tlearn: 0.0707679\ttotal: 10m 20s\tremaining: 15.9s\n","975:\tlearn: 0.0706315\ttotal: 10m 21s\tremaining: 15.3s\n","976:\tlearn: 0.0706027\ttotal: 10m 22s\tremaining: 14.6s\n","977:\tlearn: 0.0705659\ttotal: 10m 22s\tremaining: 14s\n","978:\tlearn: 0.0705216\ttotal: 10m 23s\tremaining: 13.4s\n","979:\tlearn: 0.0704693\ttotal: 10m 24s\tremaining: 12.7s\n","980:\tlearn: 0.0704546\ttotal: 10m 24s\tremaining: 12.1s\n","981:\tlearn: 0.0704123\ttotal: 10m 25s\tremaining: 11.5s\n","982:\tlearn: 0.0703708\ttotal: 10m 25s\tremaining: 10.8s\n","983:\tlearn: 0.0703694\ttotal: 10m 26s\tremaining: 10.2s\n","984:\tlearn: 0.0703356\ttotal: 10m 27s\tremaining: 9.55s\n","985:\tlearn: 0.0703003\ttotal: 10m 27s\tremaining: 8.91s\n","986:\tlearn: 0.0702700\ttotal: 10m 28s\tremaining: 8.28s\n","987:\tlearn: 0.0701406\ttotal: 10m 29s\tremaining: 7.64s\n","988:\tlearn: 0.0701063\ttotal: 10m 29s\tremaining: 7s\n","989:\tlearn: 0.0700986\ttotal: 10m 30s\tremaining: 6.37s\n","990:\tlearn: 0.0700917\ttotal: 10m 30s\tremaining: 5.73s\n","991:\tlearn: 0.0700742\ttotal: 10m 31s\tremaining: 5.09s\n","992:\tlearn: 0.0699588\ttotal: 10m 32s\tremaining: 4.46s\n","993:\tlearn: 0.0699338\ttotal: 10m 32s\tremaining: 3.82s\n","994:\tlearn: 0.0698597\ttotal: 10m 33s\tremaining: 3.18s\n","995:\tlearn: 0.0697998\ttotal: 10m 34s\tremaining: 2.55s\n","996:\tlearn: 0.0697581\ttotal: 10m 34s\tremaining: 1.91s\n","997:\tlearn: 0.0697079\ttotal: 10m 35s\tremaining: 1.27s\n","998:\tlearn: 0.0696461\ttotal: 10m 35s\tremaining: 637ms\n","999:\tlearn: 0.0695801\ttotal: 10m 36s\tremaining: 0us\n","Accuracy:  0.9703\n"]}],"source":["#Entreno el modelo de catboost\n","from catboost import CatBoostClassifier\n","cat_clf=CatBoostClassifier()\n","#entreno\n","cat_clf.fit(X_train,y_train)\n","#predigo\n","y_pred_cat=cat_clf.predict(X_val)\n","#accuracy\n","accuracy_cat=accuracy_score(y_val, y_pred_cat)\n","print(\"Accuracy: \",accuracy_cat)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9777\n"]}],"source":["import xgboost as xgb\n","from sklearn.metrics import accuracy_score\n","\n","# Entreno el modelo de XGBoost\n","xgb_clf = xgb.XGBClassifier(n_estimators=100, eval_metric='logloss')\n","\n","# Entreno el modelo\n","xgb_clf.fit(X_train, y_train)\n","\n","# Predigo en el conjunto de validación\n","y_pred_xgb = xgb_clf.predict(X_val)\n","\n","# Calculo la precisión (accuracy)\n","accuracy_xgb = accuracy_score(y_val, y_pred_xgb)\n","\n","# Imprimo la precisión\n","print(\"Accuracy:\", accuracy_xgb)\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["#Ahora uso voting classifier con los modelos entrenados\n","voting_clf_soft2=VotingClassifier(\n","    estimators=[('lgt',lgt),('cat',cat_clf),('xvg',xgb_clf)],voting='soft')\n","\n","#Entreno el voting classifier\n","voting_clf_soft2.fit(X_train,y_train)\n","\n","#predigo\n","y_pred_voting_soft2=voting_clf_soft2.predict(X_val)\n","#Accuracy\n","accuracy_voting_soft2=accuracy_score(y_val, y_pred_voting_soft2)\n","print(\"Accuracy: \",accuracy_voting_soft2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Ejercicio 2\n","from sklearn.datasets import fetch_openml\n","X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n","\n","#Divido en entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","#importo time para calcular el tiempo\n","#entreno random forest\n","rnd_clf2 = RandomForestClassifier()\n","start_time=time.time()\n","rnd_clf2.fit(X_train, y_train)\n","end_time=time.time()\n","\n","#predigo\n","y_pred_rnd2=rnd_clf2.predict(X_test)\n","#accuracy\n","accuracy_rnd2=accuracy_score(y_test, y_pred_rnd2)\n","\n","print(\"Tiempo de entrenamiento del clasificador Random Forest:\", end_time - start_time)\n","print(\"Accuracy: \",accuracy_rnd2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Uso PCA para reducir la dimensionalidad, sirve para reducir el \n","# tiempo de entrenamiento\n","\n","from sklearn.decomposition import PCA\n","pca=PCA(n_components=0.95)\n","X_train_reduced=pca.fit_transform(X_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#ahora entreno random forest pero con el conjunto de entrenamiento reducido\n","rnd_clf_reduced=RandomForestClassifier()\n","\n","start_time=time.time()\n","rnd_clf_reduced.fit(X_train_reduced, y_train)\n","end_time=time.time()\n","\n","X_test_reduced=pca.transform(X_test)\n","#predigo\n","y_pred_rnd_reduced=rnd_clf_reduced.predict(X_test_reduced)\n","print(\"Tiempo de entrenamiento del clasificador Random Forest con PCA:\", end_time - start_time)\n","print(\"Accuracy: \",accuracy_score(y_test, y_pred_rnd_reduced))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#2.2\n","#importo TSNE\n","from sklearn.manifold import TSNE\n","tsne=TSNE(n_components=2)\n","#transformo el conjunto de entrenamiento\n","X_train_TNSE=tsne.fit_transform(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Ahora realizo un grafico de dispersion con las clases de 0 al 9 con tsne\n","import matplotlib.pyplot as plt\n","\n","#grafico de dispersion\n","plt.scatter(X_train_TNSE[:,0],X_train_TNSE[:,1],c=y_train.astype(int),cmap=\"jet\")\n","plt.axis('off')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Ahora realizo un grafico de dispersion con las clases de 0 al 9 con psa\n","import matplotlib.pyplot as plt\n","\n","#grafico de dispersion\n","plt.scatter(X_train_reduced[:,0],X_train_reduced[:,1],c=y_train.astype(int),cmap=\"jet\")\n","plt.axis('off')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Ahora hacer un grafico con un conjunto de datos reducido\n","from sklearn.manifold import MDS\n","X_reduced=X[:1000]\n","y_reduced=y[:1000]\n","\n","#uso mds\n","mds=MDS(n_components=2)\n","X_train_mds=mds.fit_transform(X_reduced)\n","\n","#Genero grafico de dispersion con el conjunto de datos reducidos\n","#grafico de dispersion\n","plt.scatter(X_train_mds[:,0],X_train_mds[:,1],c=y_reduced.astype(int),cmap=\"jet\")\n","plt.axis('off')\n","plt.colorbar()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uuSAUBYkyO1i"},"source":["# Preguntas teóricas\n","### Métodos de ensamble\n","\n","1. Si ha entrenado cinco modelos diferentes en los mismos datos de entrenamiento y todos logran una precisión del 95%, ¿existe alguna posibilidad de combinar estos modelos para obtener mejores resultados? Si es así, ¿cómo? Si no, ¿por qué?\n","2. ¿Cuál es la diferencia entre los clasificadores de votación hard y de votación soft?\n","3. ¿Es posible acelerar el entrenamiento de un conjunto de bagging distribuyéndolo en varios servidores? ¿Qué pasa con los conjuntos de pasting, los conjuntos de boosting, los Random Forest o los ensambles Stacking?\n","4. ¿Cuál es el beneficio de la evaluación out-of-bag (OOB)?\n","5. ¿Qué hace que los Extra-Trees sean más aleatorios que los Random Forest regulares? ¿Cómo puede esta aleatoriedad adicional ayudar? ¿Son los Extra-Trees más lentos o más rápidos que los Random Forest regulares?\n","6. Si su conjunto de AdaBoost no se ajusta lo suficientemente bien a los datos de entrenamiento, ¿qué hiperparámetros debe ajustar y cómo?\n","7. Si su conjunto de Gradient Boosting sobreajusta el conjunto de entrenamiento, ¿debería aumentar o disminuir la tasa de aprendizaje?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","Métodos de ensamble\n","\n","    Si ha entrenado cinco modelos diferentes en los mismos datos de entrenamiento y todos logran una precisión del 95%, ¿existe alguna posibilidad de combinar estos modelos para obtener mejores resultados? Si es así, ¿cómo? Si no, ¿por qué?\n","\n","Sí, existe la posibilidad de combinar estos modelos para obtener mejores resultados utilizando métodos de ensamble. Por ejemplo, se podría utilizar un clasificador de votación soft para combinar las predicciones de los cinco modelos, lo que podría aumentar la precisión del modelo combinado. Esto se debe a que los diferentes modelos pueden capturar diferentes aspectos de la relación entre las características y las etiquetas, y al combinar las predicciones de los modelos, se puede reducir el error de generalización.\n","\n","    ¿Cuál es la diferencia entre los clasificadores de votación hard y de votación soft?\n","\n","Un clasificador de votación hard elige la clase con mayor cantidad de votos entre los clasificadores base, mientras que un clasificador de votación soft utiliza las probabilidades de clase estimadas por cada clasificador base para hacer una predicción. En otras palabras, un clasificador de votación hard cuenta los votos, mientras que un clasificador de votación soft toma en cuenta la confianza de cada modelo base.\n","\n","    ¿Es posible acelerar el entrenamiento de un conjunto de bagging distribuyéndolo en varios servidores? ¿Qué pasa con los conjuntos de pasting, los conjuntos de boosting, los Random Forest o los ensambles Stacking?\n","\n","Sí, es posible acelerar el entrenamiento de un conjunto de bagging distribuyéndolo en varios servidores. Cada servidor podría entrenar un modelo base en un subconjunto aleatorio de los datos de entrenamiento y luego combinar los modelos base para crear el modelo final. Esta técnica se conoce como bagging paralelo. Lo mismo se puede aplicar a los conjuntos de pasting, Random Forest y ensambles Stacking. Sin embargo, la aceleración del entrenamiento en el boosting es más difícil debido a que cada modelo base se entrena en función de los errores del modelo anterior, lo que hace que la distribución en múltiples servidores sea complicada.\n","\n","    ¿Cuál es el beneficio de la evaluación out-of-bag (OOB)?\n","\n","La evaluación out-of-bag (OOB) es una técnica utilizada en el bagging que permite evaluar el modelo sin la necesidad de un conjunto de validación. Durante el entrenamiento, se utiliza una muestra aleatoria del conjunto de entrenamiento para cada modelo base, lo que implica que algunas instancias no se utilizan en el entrenamiento de cada modelo. Estas instancias no utilizadas se llaman instancias out-of-bag y se pueden utilizar para evaluar el modelo. La evaluación OOB proporciona una estimación sin sesgo del rendimiento del modelo en datos no vistos.\n","\n","    ¿Qué hace que los Extra-Trees sean más aleatorios que los Random Forest regulares? ¿Cómo puede esta aleatoriedad adicional ayudar? ¿Son los Extra-Trees más lentos o más rápidos que los Random Forest regulares?\n","\n","Los Extra-Trees (Extremely Randomized Trees) son más aleatorios que los Random Forest regulares porque en lugar de seleccionar las características óptimas para dividir cada nodo, utilizan una selección aleatoria de características. Además, las divisiones de nodos se realizan en puntos aleatorios para cada característica. Esta aleatoriedad adicional ayuda a reducir la varianza del modelo, lo que puede mejorar su rendimiento en datos no vistos. Los Extra-Trees son generalmente más rápidos que los Random Forest regulares, ya que la selección aleatoria de características reduce el tiempo necesario para encontrar la mejor característica para cada nodo.\n","\n","    Si su conjunto de AdaBoost no se ajusta lo suficientemente bien a los datos de entrenamiento, ¿qué hiperparámetros debe ajustar y cómo?\n","\n","Si el conjunto de AdaBoost no se ajusta lo suficientemente bien a los datos de entrenamiento, se pueden ajustar varios hiperparámetros, como la tasa de aprendizaje (learning rate) y el número de estimadores (n_estimators).\n","\n","Aumentar la tasa de aprendizaje (learning rate) puede ayudar a hacer que el conjunto sea más sensible a los errores del modelo anterior y, por lo tanto, a reducir el sesgo. Aumentar el número de estimadores también puede mejorar el rendimiento del conjunto, permitiendo que el modelo tenga más oportunidades de ajustarse a los datos. También se pueden ajustar otros hiperparámetros, como el criterio de división de nodos (splitter) y la profundidad máxima del árbol (max_depth), para mejorar el rendimiento.\n","\n","    Si su conjunto de Gradient Boosting sobreajusta el conjunto de entrenamiento, ¿debería aumentar o disminuir la tasa de aprendizaje?\n","\n","Si el conjunto de Gradient Boosting sobreajusta el conjunto de entrenamiento, debería disminuir la tasa de aprendizaje para reducir la velocidad a la que se ajusta el modelo a los errores del modelo anterior. Al disminuir la tasa de aprendizaje, el modelo tendrá menos oportunidades de sobreajustar el conjunto de entrenamiento y, por lo tanto, debería generalizar mejor a los datos no vistos. También se pueden considerar otras técnicas para reducir el sobreajuste, como reducir la profundidad de los árboles o utilizar una muestra aleatoria de las características en cada iteración.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","### Reducción dimensional\n","1. ¿Cuáles son las principales motivaciones para reducir la dimensionalidad de un conjunto de datos? ¿Cuáles son las principales desventajas?\n","2. ¿A qué se denomina la maldición de la dimensionalidad?\n","3. Una vez que se ha reducido la dimensionalidad de un conjunto de datos, ¿es posible revertir la operación? Si es así, ¿cómo? Si no, ¿por qué no?\n","4. ¿Se puede utilizar PCA para reducir la dimensionalidad de un conjunto de datos altamente no lineal?\n","5. Suponga que realiza PCA en un conjunto de datos de 1,000 dimensiones, estableciendo la relación de varianza explicada en un 95%. ¿Cuántas dimensiones tendrá el conjunto de datos resultante?\n","6. ¿En qué casos utilizaría PCA simple, PCA incremental, PCA aleatorio o kernel PCA?\n","7. ¿Cómo se puede evaluar el rendimiento de un algoritmo de reducción de dimensionalidad en su conjunto de datos?\n","8. ¿Tiene sentido encadenar dos algoritmos de reducción de dimensionalidad diferentes?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","1-Las principales motivaciones para reducir la dimensionalidad de un conjunto de datos incluyen la reducción del tiempo de cálculo, la reducción del riesgo de sobreajuste y la mejora de la interpretabilidad de los datos. Las principales desventajas incluyen la posible pérdida de información y la posibilidad de introducir ruido o distorsiones no deseadas en los datos.\n","\n","2-La maldición de la dimensionalidad se refiere al fenómeno en el que la dificultad de analizar datos aumenta exponencialmente con el número de dimensiones.\n","\n","3-Sí, es posible revertir la operación de reducir la dimensionalidad de un conjunto de datos. Esto se puede hacer aplicando una transformación inversa a los datos reducidos, utilizando el mismo método que se utilizó para reducir la dimensionalidad en primer lugar.\n","\n","4-Sí, PCA se puede usar para reducir la dimensionalidad de un conjunto de datos altamente no lineal, aunque puede que no sea el método más efectivo en este caso. Otros métodos, como el PCA kernel o el t-SNE, pueden ser más apropiados para conjuntos de datos altamente no lineales.\n","\n","5-El número de dimensiones en el conjunto de datos resultante dependerá del número de componentes principales necesarios para explicar el 95% de la varianza en los datos. Este número probablemente será menor que 1000, pero el número exacto dependerá del conjunto de datos específico.\n","\n","6-El PCA \"vanilla\" es apropiado para conjuntos de datos con una estructura lineal y una dimensionalidad moderada a alta. El PCA incremental es útil para conjuntos de datos grandes que no se pueden cargar en memoria de una sola vez. El PCA aleatorio es útil para conjuntos de datos grandes con muchas características, donde la solución exacta es computacionalmente inviable. El PCA kernel es apropiado para conjuntos de datos no lineales.\n","\n","7-El rendimiento de un algoritmo de reducción de dimensionalidad se puede evaluar midiendo cómo preserva la estructura de los datos. Esto se puede hacer comparando los datos reducidos con los datos originales utilizando métricas como la relación de varianza explicada o el error de reconstrucción.\n","\n","8-Encadenar dos algoritmos de reducción de dimensionalidad diferentes a veces puede ser beneficioso, especialmente cuando cada método es más adecuado para un aspecto diferente de los datos. Sin embargo, se debe tener cuidado para garantizar que los datos reducidos resultantes sigan siendo interpretables y útiles para el análisis previsto"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
